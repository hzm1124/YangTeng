{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "591653ce-ad0b-44d5-bc09-ad28c6d356d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总数量：2\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "fuck?:  fuck\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[尝试次数：1] - 1.https://www.amazon.com/gp/bestsellers/automotive/15730511/ref=pd_zg_hrsr_automotive?language=en_US&pg=1 > Page 1\n",
      "[剩余数量：1] - [当前时间：11:08:19]\n",
      "\n",
      "[尝试次数：1] - 1.https://www.amazon.com/gp/bestsellers/automotive/15730511/ref=pd_zg_hrsr_automotive?language=en_US&pg=2 > Page 2\n",
      "[剩余数量：1] - [当前时间：11:08:21]\n",
      "\n",
      "[尝试次数：1] - 2.https://www.amazon.com/gp/bestsellers/wireless/7072562011/ref=pd_zg_hrsr_wireless?language=en_US&pg=1 > Page 1\n",
      "[剩余数量：0] - [当前时间：11:08:39]\n",
      "\n",
      "[尝试次数：1] - 2.https://www.amazon.com/gp/bestsellers/wireless/7072562011/ref=pd_zg_hrsr_wireless?language=en_US&pg=2 > Page 2\n",
      "[剩余数量：0] - [当前时间：11:08:40]\n",
      "\n",
      "输出ing...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████████████████████████| 2/2 [00:00<00:00, 2853.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "if os.path.exists('./file/1.url'):\n",
    "    shutil.rmtree('./file/1.url')\n",
    "\n",
    "os.makedirs('./file/1.url')\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "from selenium.webdriver import ChromeOptions\n",
    "\n",
    "import sys\n",
    "sys.path.append('../00.Tools')\n",
    "from crawler_configuration import get_header, get_proxy\n",
    "\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "\n",
    "import json\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "output_error = pd.DataFrame()\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "input_ = pd.read_excel('./file/input.xlsx',\n",
    "                       header=0,\n",
    "                       dtype=str).fillna('')\n",
    "\n",
    "input_['No'] = input_['No'].astype(int)\n",
    "\n",
    "print(f'总数量：{len(input_)}')\n",
    "print()\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "option = ChromeOptions()\n",
    "option.add_experimental_option('useAutomationExtension', False)\n",
    "option.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "option.add_argument('--disable-blink-features=AutomationControlled')\n",
    "option.add_argument('--disable-dev-shm-usage')\n",
    "option.add_argument('--disable-gpu')\n",
    "option.add_argument('--no-sandbox')\n",
    "# option.add_argument('--headless=new')\n",
    "# option.add_argument('blink-settings=imagesEnabled=false')\n",
    "option.add_argument(get_header(method='selenium'))\n",
    "option.add_argument(get_proxy(method='selenium'))\n",
    "option.page_load_strategy = 'eager'\n",
    "\n",
    "browser = Chrome(service=Service('../chromedriver'), options=option)\n",
    "browser.maximize_window()\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "b = 0\n",
    "while True:\n",
    "    b += 1\n",
    "\n",
    "    try:\n",
    "        browser.get('https://www.amazon.com/?language=en_US')\n",
    "        \n",
    "        break\n",
    "    except KeyboardInterrupt:\n",
    "        browser.quit()\n",
    "        break\n",
    "    except:\n",
    "        browser.quit()\n",
    "        continue\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "input(f'''fuck?: ''')\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "for a in range(len(input_)):\n",
    "\n",
    "    # = = = = = = = = = = = = = = =\n",
    "\n",
    "    crawler_status = 'error'\n",
    "\n",
    "    # = = = = = = = = = = = = = = =\n",
    "\n",
    "    try:\n",
    "        url = input_.loc[a, 'Url']\n",
    "        \n",
    "        dict_param = {}\n",
    "        list_param = url.split('?')[1].split('&') if '?' in url else []\n",
    "        for param in list_param:\n",
    "            dict_param[param.split('=')[0]] = param.split('=')[1]\n",
    "        \n",
    "        dict_param.pop('pg', '')\n",
    "        dict_param['language'] = 'en_US'\n",
    "        \n",
    "        url = url.split('?')[0] + '?'\n",
    "        for key, value in dict_param.items():\n",
    "            url += f'{key}={value}&'\n",
    "        \n",
    "        list_url, page = [], 1\n",
    "        while True:\n",
    "            b = 0\n",
    "            while True:\n",
    "                b += 1\n",
    "\n",
    "                try:\n",
    "                    request_url = f'{url}pg={page}'\n",
    "                    \n",
    "                    browser.get(request_url)\n",
    "\n",
    "                    soup = BeautifulSoup(browser.page_source, 'lxml')\n",
    "                    html = etree.HTML(str(soup))\n",
    "\n",
    "                    list_dict = json.loads(html.xpath('//div[@data-client-recs-list]/@data-client-recs-list')[0])\n",
    "        \n",
    "                    if list_dict:\n",
    "                        break\n",
    "                except KeyboardInterrupt:\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "            # = = = = = = = = = = = = = = =\n",
    "\n",
    "            list_url += [f'''https://www.amazon.com/dp/{dict_['id']}?language=en_US''' for dict_ in list_dict]\n",
    "        \n",
    "            # = = = = = = = = = = = = = = =\n",
    "\n",
    "            if a % 100 == 0 and a != 0:\n",
    "                clear_output()\n",
    "\n",
    "            print(f'''[尝试次数：{b}] - {input_.loc[a, 'No']}.{request_url} > Page {page}\\n[剩余数量：{len(input_) - a - 1}] - [当前时间：{datetime.now().strftime('%H:%M:%S')}]\\n''')\n",
    "\n",
    "            # = = = = = = = = = = = = = = =\n",
    "\n",
    "            page += 1\n",
    "        \n",
    "            # = = = = = = = = = = = = = = =\n",
    "\n",
    "            if html.xpath('//li[@class=\"a-disabled a-last\"]'):\n",
    "                break\n",
    "        \n",
    "        # = = = = = = = = = = = = = = =\n",
    "\n",
    "        if not list_url:\n",
    "            raise\n",
    "\n",
    "        # = = = = = = = = = = = = = = =\n",
    "\n",
    "        output = pd.DataFrame({'No': [i+1 for i in range(len(list_url))],\n",
    "                               'Url': list_url})\n",
    "        \n",
    "        output.to_excel(f'''./file/1.url/crawler_{input_.loc[a, 'No']}-{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx''', index=False)\n",
    "\n",
    "        # = = = = = = = = = = = = = = =\n",
    "\n",
    "        crawler_status = 'ok'\n",
    "    except:\n",
    "        df_temp = pd.DataFrame([input_.iloc[a]]).reset_index(drop=True)\n",
    "        df_temp.loc[0, 'Request_Url'] = request_url\n",
    "\n",
    "        output_error = pd.concat([output_error, df_temp], ignore_index=True).fillna('')\n",
    "        \n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "print('输出ing...')\n",
    "print()\n",
    "if not output_error.empty:\n",
    "    output_error = output_error.sort_values(by=['No'],\n",
    "                                            ascending=[True],\n",
    "                                            ignore_index=True)\n",
    "    output_error.to_excel(f'''./file/url_error.xlsx''', index=False)\n",
    "    print('爬虫存在error')\n",
    "    print()\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "list_file = sorted(list(os.walk('./file/1.url'))[0][2])\n",
    "while '.DS_Store' in list_file:\n",
    "    list_file.remove('.DS_Store')\n",
    "while '0.null.txt' in list_file:\n",
    "    list_file.remove('0.null.txt')\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "for file in tqdm(list_file, desc='Progress', ncols=77):\n",
    "    os.rename(f'./file/1.url/{file}',\n",
    "              f'''./file/1.url/{re.sub(r'-[0-9]{8}_[0-9]{6}.xlsx$', '.xlsx', file)}''')\n",
    "\n",
    "print('Done ~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c7223f-b990-41ab-b7a7-2733c9661412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
