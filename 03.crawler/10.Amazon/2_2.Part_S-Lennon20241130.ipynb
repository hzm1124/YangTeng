{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3549c21d-85e3-432c-b31e-0f7bea25694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "\n",
    "# = = =  = = =  = = =\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# = = =  = = =  = = =\n",
    "\n",
    "from selenium.webdriver import ChromeOptions\n",
    "\n",
    "import sys\n",
    "sys.path.append('../00.Tools')\n",
    "from crawler_configuration import get_header, get_proxy\n",
    "\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "import time\n",
    "\n",
    "# = = =  = = =  = = =\n",
    "\n",
    "from urllib.parse import urlparse, parse_qs, urlencode\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "\n",
    "# = = =  = = =  = = =\n",
    "\n",
    "import re\n",
    "\n",
    "# = = =  = = =  = = =\n",
    "\n",
    "import json\n",
    "\n",
    "# = = =  = = =  = = =\n",
    "\n",
    "import html as html_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e14ec94-37b0-4cb6-b065-12820ddd3a5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loading...\n",
      "Loading Done !\n",
      "\n",
      "总数量：2\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Url</th>\n",
       "      <th>Request_Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>https://www.amazon.com/dp/B00HQ2CCCE</td>\n",
       "      <td>https://www.amazon.com/dp/B00HQ2CCCE?language=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>https://www.amazon.com/dp/B0C3ML1NDC</td>\n",
       "      <td>https://www.amazon.com/dp/B0C3ML1NDC?language=...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   No                                   Url  \\\n",
       "0  20  https://www.amazon.com/dp/B00HQ2CCCE   \n",
       "1  81  https://www.amazon.com/dp/B0C3ML1NDC   \n",
       "\n",
       "                                         Request_Url  \n",
       "0  https://www.amazon.com/dp/B00HQ2CCCE?language=...  \n",
       "1  https://www.amazon.com/dp/B0C3ML1NDC?language=...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Data Loading...')\n",
    "list_dict_correct, list_series_error, df_input = [], [], pd.read_excel('./file/1.url.xlsx',\n",
    "                                                                       header=0,\n",
    "                                                                       dtype=str).fillna('')\n",
    "print('Loading Done !')\n",
    "print()\n",
    "\n",
    "df_input['No'] = df_input['No'].astype(int)\n",
    "\n",
    "crawler_remain = len(df_input)\n",
    "\n",
    "print(f'总数量：{crawler_remain}')\n",
    "print()\n",
    "\n",
    "df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "683123d4-59ce-4401-a4a0-09ca203ed98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.chrome.webdriver.WebDriver (session=\"350cae58dfed37274fdad77b8424a038\")>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "option = ChromeOptions()\n",
    "option.add_experimental_option('useAutomationExtension', False)\n",
    "option.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "option.add_argument('--disable-blink-features=AutomationControlled')\n",
    "option.add_argument('--disable-dev-shm-usage')\n",
    "option.add_argument('--disable-gpu')\n",
    "option.add_argument('--no-sandbox')\n",
    "# option.add_argument('--headless=new')\n",
    "# option.add_argument('blink-settings=imagesEnabled=false')\n",
    "option.add_argument(get_header(method='selenium'))\n",
    "option.add_argument(get_proxy(method='selenium'))\n",
    "option.page_load_strategy = 'normal'\n",
    "\n",
    "driver = Chrome(service=Service('../chromedriver'), options=option)\n",
    "driver.maximize_window()\n",
    "\n",
    "# = = =  = = =  = = =\n",
    "\n",
    "crawler_retry = 0\n",
    "while True:\n",
    "    crawler_retry += 1\n",
    "    print(crawler_retry)\n",
    "    try:\n",
    "        driver.get('https://www.amazon.com/?language=en_US')\n",
    "        time.sleep(1)\n",
    "        driver.execute_script(\"window.scrollTo(0, 0.2 * document.body.scrollHeight);\")\n",
    "        time.sleep(1.5)\n",
    "        driver.execute_script(\"window.scrollTo(0, 0.45 * document.body.scrollHeight);\")\n",
    "        time.sleep(0.7)\n",
    "        driver.execute_script(\"window.scrollTo(0, 0.7 * document.body.scrollHeight);\")\n",
    "        time.sleep(1.2)\n",
    "        driver.execute_script(\"window.scrollTo(0, 0.95 * document.body.scrollHeight);\")\n",
    "        time.sleep(1)\n",
    "        driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "\n",
    "        break\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    except:\n",
    "        time.sleep(3)\n",
    "        continue\n",
    "\n",
    "driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82a24562-0d61-4c80-bc43-edc4ed37c49c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True] - [请求1次] - [剩余1条] - [2024/11/30 15:52:47] - 20. https://www.amazon.com/dp/B00HQ2CCCE?language=en_US\n",
      "\n",
      "[True] - [请求1次] - [剩余0条] - [2024/11/30 15:52:55] - 81. https://www.amazon.com/dp/B0C3ML1NDC?language=en_US\n",
      "\n",
      "Data Outputting...\n",
      "Output Finished !\n",
      "\n",
      "Done ~\n"
     ]
    }
   ],
   "source": [
    "for crawler_index, crawler_series in df_input.iterrows():\n",
    "\n",
    "    crawler_status = False\n",
    "\n",
    "    # = = =  = = =  = = =\n",
    "\n",
    "    try:\n",
    "        url_parse = urlparse(crawler_series['Url'])\n",
    "\n",
    "        dict_param = parse_qs(url_parse.query)\n",
    "        dict_param['language'] = 'en_US'\n",
    "\n",
    "        url_request = f'{url_parse.scheme}://{url_parse.netloc}{url_parse.path}?{urlencode(dict_param, doseq=True)}'\n",
    "\n",
    "        crawler_retry = 0\n",
    "        while True:\n",
    "            crawler_retry += 1\n",
    "\n",
    "            try:\n",
    "                driver.get(url_request)\n",
    "                time.sleep(1)\n",
    "                driver.execute_script(\"window.scrollTo(0, 0.2 * document.body.scrollHeight);\")\n",
    "                time.sleep(1.5)\n",
    "                driver.execute_script(\"window.scrollTo(0, 0.45 * document.body.scrollHeight);\")\n",
    "                time.sleep(0.7)\n",
    "                driver.execute_script(\"window.scrollTo(0, 0.7 * document.body.scrollHeight);\")\n",
    "                time.sleep(1.2)\n",
    "                driver.execute_script(\"window.scrollTo(0, 0.95 * document.body.scrollHeight);\")\n",
    "                time.sleep(1)\n",
    "                driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "                html = etree.HTML(str(soup))\n",
    "\n",
    "                break\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # = = =  = = =  = = =\n",
    "\n",
    "        zip_code = re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', html.xpath('//span[@id=\"glow-ingress-line2\"]/text()')[0]).strip()\n",
    "\n",
    "        # = = =  = = =  = = =\n",
    "\n",
    "        price = html.xpath('//span[@class=\"a-offscreen\"]/text()')[0].strip() if html.xpath('//span[@class=\"a-offscreen\"]/text()') else ''\n",
    "\n",
    "        # = = =  = = =  = = =\n",
    "\n",
    "        dict_src, list_src = {}, json.loads('{\"initial\":[{' + str(soup).split('\\'colorImages\\': { \\'initial\\': [{')[1].split('}]},')[0] + '}]}')['initial']\n",
    "        for i, src in enumerate(list_src):\n",
    "            dict_src[str(i)] = src['hiRes'].strip() if src['hiRes'] else src['large'].strip()\n",
    "\n",
    "        # = = =  = = =  = = =\n",
    "\n",
    "        dict_key, list_tr = {}, html.xpath('//table[@class=\"a-normal a-spacing-micro\"]/tbody/tr')\n",
    "        for tr in list_tr:\n",
    "            key = re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', tr.xpath('./td/span/text()')[0]).strip()\n",
    "            if key in dict_key:\n",
    "                dict_key[key] += 1\n",
    "            else:\n",
    "                dict_key[key] = 1\n",
    "\n",
    "        dict_overview, dict_count = {}, {}\n",
    "        for i, tr in enumerate(list_tr):\n",
    "            key = re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', tr.xpath('./td/span/text()')[0]).strip()\n",
    "            if dict_key[key] == 1:\n",
    "                dict_overview[str(i)] = {key: re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', tr.xpath('./td/span/text()')[1]).strip()}\n",
    "            else:\n",
    "                if key in dict_key:\n",
    "                    dict_key[key] += 1\n",
    "                else:\n",
    "                    dict_key[key] = 1\n",
    "                dict_overview[str(i)] = {f'{key}-{dict_count[key]}': re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', tr.xpath('./td/span/text()')[1]).strip()}\n",
    "\n",
    "        # = = =  = = =  = = =\n",
    "\n",
    "        about, list_text = '', [re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', text).strip() for text in html.xpath('//ul[@class=\"a-unordered-list a-vertical a-spacing-mini\"]/li/span/text()')]\n",
    "        for i, text in enumerate(list_text):\n",
    "            about += f'{i + 1}. {text}\\n'\n",
    "\n",
    "        # = = =  = = =  = = =\n",
    "\n",
    "        dict_ = json.loads(re.sub(r'\\\\(?![\"\\\\])', r'\\\\\\\\', str(soup).split('jQuery.parseJSON(\\'')[1].split('}\\');')[0] + '}'))\n",
    "\n",
    "        # = = =  = = =  = = =\n",
    "\n",
    "        title = re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', dict_['title']).strip()\n",
    "\n",
    "        # = = =  = = =  = = =\n",
    "\n",
    "        parent_asin = dict_['parentAsin'].strip()\n",
    "\n",
    "        # = = =  = = =  = = =\n",
    "\n",
    "        dict_key, list_tr = {}, html.xpath('//table[@id=\"productDetails_techSpec_section_1\"]/tbody/tr')\n",
    "        for tr in list_tr:\n",
    "            key = re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', tr.xpath('./th/text()')[0]).strip()\n",
    "            if key in dict_key:\n",
    "                dict_key[key] += 1\n",
    "            else:\n",
    "                dict_key[key] = 1\n",
    "\n",
    "        dict_technical, dict_count = {}, {}\n",
    "        for i, tr in enumerate(list_tr):\n",
    "            key = re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', tr.xpath('./th/text()')[0]).strip()\n",
    "            if dict_key[key] == 1:\n",
    "                dict_technical[str(i)] = {key: re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', tr.xpath('./td/text()')[0]).strip()}\n",
    "            else:\n",
    "                if key in dict_count:\n",
    "                    dict_count[key] += 1\n",
    "                else:\n",
    "                    dict_count[key] = 1\n",
    "                dict_technical[str(i)] = {f'{key}-{dict_count[key]}': re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', tr.xpath('./td/text()')[0]).strip()}\n",
    "\n",
    "        # = = =  = = =  = = =\n",
    "\n",
    "        i, dict_additional, list_tr = 0, {}, html.xpath('//table[@id=\"productDetails_detailBullets_sections1\"]/tbody/tr')\n",
    "        for tr in list_tr:\n",
    "            key = re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', tr.xpath('./th/text()')[0]).strip()\n",
    "            if key == 'Customer Reviews':\n",
    "                dict_additional[str(i)] = {'Customer Reviews Rating Count': tr.xpath('./td/div/span[@data-action=\"acrLink-click-metrics\"]/a/span/text()')[0].strip()}\n",
    "                dict_additional[str(i + 1)] = {'Customer Reviews Rating': tr.xpath('./td/div/span[@data-action=\"acrStarsLink-click-metrics\"]/span/@title')[0].strip()}\n",
    "                i += 2\n",
    "            elif key == 'Best Sellers Rank':\n",
    "                list_span = tr.xpath('./td/span/span')\n",
    "                list_text = [etree.tostring(span).decode('utf-8') for span in list_span]\n",
    "                list_text = [re.sub(r'<.*?>|\\(.*?\\)', '', text).strip() for text in list_text]\n",
    "                list_text = [re.sub(r' +', ' ', text.strip()) for text in list_text]\n",
    "                list_text = [html_lib.unescape(text) for text in list_text]\n",
    "\n",
    "                list_href = [f'https://www.amazon.com{href.strip()}' for href in tr.xpath('./td/span/span/a/@href')]\n",
    "                for j, (text, href) in enumerate(zip(list_text, list_href)):\n",
    "                    if j == 0:\n",
    "                        dict_additional[str(i)] = {'Best Sellers Rank': text}\n",
    "                        dict_additional[str(i + 1)] = {'Best Sellers Rank Url': href}\n",
    "                    else:\n",
    "                        dict_additional[str(i)] = {f'Best Sellers Rank-{j + 1}': text}\n",
    "                        dict_additional[str(i + 1)] = {f'Best Sellers Rank-{j + 1} Url': href}\n",
    "                    i += 2\n",
    "            else:\n",
    "                dict_additional[str(i)] = {key: re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', tr.xpath('./td/text()')[0]).strip()}\n",
    "                i += 1\n",
    "\n",
    "        # = = =  = = =  = = =\n",
    "\n",
    "        dict_key, list_tr = {}, html.xpath('//div[@id=\"detailBulletsWrapper_feature_div\"]/div[@id=\"detailBullets_feature_div\"]/ul/li/span')\n",
    "        for tr in list_tr:\n",
    "            key = re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', tr.xpath('./span/text()')[0]).strip().removesuffix(':').strip()\n",
    "            if key in dict_key:\n",
    "                dict_key[key] += 1\n",
    "            else:\n",
    "                dict_key[key] = 1\n",
    "\n",
    "        dict_technical_2, dict_count = {}, {}\n",
    "        for i, tr in enumerate(list_tr):\n",
    "            key = re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', tr.xpath('./span/text()')[0]).strip().removesuffix(':').strip()\n",
    "            if dict_key[key] == 1:\n",
    "                dict_technical_2[str(i)] = {key: re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', tr.xpath('./span/text()')[1]).strip()}\n",
    "            else:\n",
    "                if key in dict_count:\n",
    "                    dict_count[key] += 1\n",
    "                else:\n",
    "                    dict_count[key] = 1\n",
    "                dict_technical_2[str(i)] = {f'{key}-{dict_count[key]}': re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', tr.xpath('./span/text()')[1]).strip()}\n",
    "\n",
    "        # = = =  = = =  = = =\n",
    "\n",
    "        i, dict_additional_2, list_span = 0, {}, html.xpath('//div[@id=\"detailBulletsWrapper_feature_div\"]/ul[@class=\"a-unordered-list a-nostyle a-vertical a-spacing-none detail-bullet-list\"]/li/span')\n",
    "        for span in list_span:\n",
    "            key = re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', span.xpath('./span/text()')[0]).strip().removesuffix(':').strip()\n",
    "            if key == 'Customer Reviews':\n",
    "                dict_additional_2[str(i)] = {'Customer Reviews Rating Count': span.xpath('./div/span[@data-action=\"acrLink-click-metrics\"]/a/span/text()')[0].strip()}\n",
    "                dict_additional_2[str(i + 1)] = {'Customer Reviews Rating': span.xpath('./div/span[@data-action=\"acrStarsLink-click-metrics\"]/span/@title')[0].strip()}\n",
    "                i += 2\n",
    "            elif key == 'Best Sellers Rank':\n",
    "                list_span_2 = span.xpath('./descendant-or-self::span[@class=\"a-list-item\"]')\n",
    "                list_text = [etree.tostring(span_2).decode('utf-8') for span_2 in list_span_2]\n",
    "                list_text = [re.sub(r'<ul.*?</ul>|<span.*?Best Sellers Rank.*?</span>|<.*?>|\\(.*?\\)', '', text).strip() for text in list_text]\n",
    "                list_text = [re.sub(r' +', ' ', text.strip()) for text in list_text]\n",
    "                list_text = [html_lib.unescape(text) for text in list_text]\n",
    "\n",
    "                list_href = [f'https://www.amazon.com{href.strip()}' for href in span.xpath('./descendant-or-self::a/@href')]\n",
    "                for j, (text, href) in enumerate(zip(list_text, list_href)):\n",
    "                    if j == 0:\n",
    "                        dict_additional_2[str(i)] = {'Best Sellers Rank': text}\n",
    "                        dict_additional_2[str(i + 1)] = {'Best Sellers Rank Url': href}\n",
    "                    else:\n",
    "                        dict_additional_2[str(i)] = {f'Best Sellers Rank-{j + 1}': text}\n",
    "                        dict_additional_2[str(i + 1)] = {f'Best Sellers Rank-{j + 1} Url': href}\n",
    "                    i += 2\n",
    "            else:\n",
    "                print('fuck')\n",
    "\n",
    "        # = = =  = = =  = = =\n",
    "\n",
    "        dict_key, list_tr = {}, html.xpath('//div[@id=\"productDetails_expanderSectionTables\"]/div[@class=\"a-row\"]/div[contains(@class, \"a-column a-span6\")]/div[@class=\"a-row a-spacing-medium-plus\"]/div[@class=\"a-section\"]/div[contains(@data-csa-c-content-id, \"voyager-expander-btn\")]/div/div[@class=\"a-section\"]/table/tbody/tr')\n",
    "        for tr in list_tr:\n",
    "            key = re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', tr.xpath('./th/text()')[0]).strip().removesuffix(':').strip()\n",
    "            if key in dict_key:\n",
    "                dict_key[key] += 1\n",
    "            else:\n",
    "                dict_key[key] = 1\n",
    "\n",
    "        i, dict_information, dict_count = 0, {}, {}\n",
    "        for tr in list_tr:\n",
    "            key = re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', tr.xpath('./th/text()')[0]).strip().removesuffix(':').strip()\n",
    "            if dict_key[key] == 1:\n",
    "                if key == 'Customer Reviews':\n",
    "                    dict_information[str(i)] = {'Customer Reviews Rating Count': tr.xpath('./td/div/span[@data-action=\"acrLink-click-metrics\"]/a/span/text()')[0].strip()}\n",
    "                    dict_information[str(i + 1)] = {'Customer Reviews Rating': tr.xpath('./td/div/span[@data-action=\"acrStarsLink-click-metrics\"]/span/@title')[0].strip()}\n",
    "                    i += 2\n",
    "                elif key == 'Best Sellers Rank':\n",
    "                    list_span = tr.xpath('./td/span/span')\n",
    "                    list_text = [etree.tostring(span).decode('utf-8') for span in list_span]\n",
    "                    list_text = [re.sub(r'<.*?>|\\(.*?\\)', '', text).strip() for text in list_text]\n",
    "                    list_text = [re.sub(r' +', ' ', text.strip()) for text in list_text]\n",
    "                    list_text = [html_lib.unescape(text) for text in list_text]\n",
    "\n",
    "                    list_href = [f'https://www.amazon.com{href.strip()}' for href in tr.xpath('./td/span/span/a/@href')]\n",
    "                    for j, (text, href) in enumerate(zip(list_text, list_href)):\n",
    "                        if j == 0:\n",
    "                            dict_information[str(i)] = {'Best Sellers Rank': text}\n",
    "                            dict_information[str(i + 1)] = {'Best Sellers Rank Url': href}\n",
    "                        else:\n",
    "                            dict_information[str(i)] = {f'Best Sellers Rank-{j + 1}': text}\n",
    "                            dict_information[str(i + 1)] = {f'Best Sellers Rank-{j + 1} Url': href}\n",
    "                        i += 2\n",
    "                else:\n",
    "                    dict_information[str(i)] = {key: re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', tr.xpath('./td/text()')[0]).strip()}\n",
    "                    i += 1\n",
    "            else:\n",
    "                if key in dict_count:\n",
    "                    dict_count[key] += 1\n",
    "                else:\n",
    "                    dict_count[key] = 1\n",
    "                dict_information[str(i)] = {f'{key}-{dict_count[key]}': re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', tr.xpath('./td/text()')[0]).strip()}\n",
    "\n",
    "        # = = =  = = =  = = =\n",
    "\n",
    "        if not dict_technical and not dict_additional and not dict_technical_2 and not dict_additional_2 and not dict_information:\n",
    "            raise\n",
    "\n",
    "        # = = =  = = =  = = =\n",
    "\n",
    "        list_dict_correct.append({'No': crawler_series['No'],\n",
    "                                  'Title': title,\n",
    "                                  'About': about.strip(),\n",
    "                                  'Price': price,\n",
    "                                  'Picture': '',\n",
    "                                  'Url': url_request,\n",
    "                                  'Zip Code': zip_code,\n",
    "                                  'Parent ASIN': parent_asin,\n",
    "                                  'Json_Src': json.dumps(dict_src),\n",
    "                                  'Json_Overview': json.dumps(dict_overview),\n",
    "                                  'Json_Technical': json.dumps(dict_technical),\n",
    "                                  'Json_Additional': json.dumps(dict_additional),\n",
    "                                  'Json_Technical 2': json.dumps(dict_technical_2),\n",
    "                                  'Json_Additional 2': json.dumps(dict_additional_2),\n",
    "                                  'Json_Information': json.dumps(dict_information)})\n",
    "\n",
    "        # = = =  = = =  = = =\n",
    "\n",
    "        crawler_status = True\n",
    "    except:\n",
    "        crawler_series['Request_Url'] = url_request\n",
    "\n",
    "        list_series_error.append(crawler_series)\n",
    "\n",
    "    # = = =  = = =  = = =\n",
    "\n",
    "    if crawler_remain % 30 == 0:\n",
    "        clear_output()\n",
    "    print(f'''[{crawler_status}] - [请求{crawler_retry}次] - [剩余{crawler_remain - 1}条] - [{datetime.now().strftime('%Y/%m/%d %H:%M:%S')}] - {crawler_series['No']}. {url_request}\\n''')\n",
    "\n",
    "    crawler_remain -= 1\n",
    "\n",
    "# = = =  = = =  = = =\n",
    "\n",
    "print('Data Outputting...')\n",
    "if list_dict_correct:\n",
    "    df_correct = pd.DataFrame(list_dict_correct).sort_values(by=['No'],\n",
    "                                                             ascending=[True],\n",
    "                                                             ignore_index=True)\n",
    "    df_correct.to_excel(f'''./file/2.part-{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx''', index=False)\n",
    "print('Output Finished !')\n",
    "print()\n",
    "if list_series_error:\n",
    "    df_error = pd.DataFrame(list_series_error).sort_values(by=['No'],\n",
    "                                                           ascending=[True],\n",
    "                                                           ignore_index=True)\n",
    "    df_error.to_excel('./file/page_error.xlsx', index=False)\n",
    "    print('- Error Occurred -')\n",
    "    print()\n",
    "\n",
    "# = = =  = = =  = = =\n",
    "\n",
    "driver.quit()\n",
    "print('Done ~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a73d55-f26c-466a-89d7-529fb5ec190b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
