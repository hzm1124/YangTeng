{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b786d7ed-adec-45a0-a457-744c96ab0d7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总数量：10\n",
      "\n",
      "[状态：ok，尝试次数：1] - 50.00% - crawler_1 > 1.https://www.rockauto.com/en/parts/melling,30250S,timing+chain+&+component+kit,5756\n",
      "[剩余数量：5] - [当前时间：19:03:26]\n",
      "\n",
      "[状态：ok，尝试次数：1] - 50.00% - crawler_1 > 2.https://www.rockauto.com/en/parts/melling,30250SX,timing+chain+&+component+kit,5756\n",
      "[剩余数量：4] - [当前时间：19:03:27]\n",
      "\n",
      "[状态：ok，尝试次数：1] - 50.00% - crawler_1 > 6.https://www.rockauto.com/en/parts/melling,30460SA,timing+chain+&+component+kit,5756\n",
      "[剩余数量：3] - [当前时间：19:03:27]\n",
      "\n",
      "[状态：ok，尝试次数：1] - 50.00% - crawler_1 > 8.https://www.rockauto.com/en/parts/melling,30470S,timing+chain+&+component+kit,5756\n",
      "[剩余数量：2] - [当前时间：19:03:29]\n",
      "\n",
      "[状态：ok，尝试次数：1] - 50.00% - crawler_1 > 7.https://www.rockauto.com/en/parts/melling,30460SB,timing+chain+&+component+kit,5756\n",
      "[剩余数量：1] - [当前时间：19:03:30]\n",
      "\n",
      "[状态：ok，尝试次数：2] - 50.00% - crawler_1 > 4.https://www.rockauto.com/en/parts/melling,30430S,timing+chain+&+component+kit,5756\n",
      "[剩余数量：0] - [当前时间：19:03:30]\n",
      "\n",
      "[状态：ok，尝试次数：2] - 50.00% - crawler_1 > 5.https://www.rockauto.com/en/parts/melling,30460S,timing+chain+&+component+kit,5756\n",
      "[剩余数量：0] - [当前时间：19:03:32]\n",
      "\n",
      "[状态：ok，尝试次数：2] - 50.00% - crawler_1 > 9.https://www.rockauto.com/en/parts/melling,30470SX,timing+chain+&+component+kit,5756\n",
      "[剩余数量：0] - [当前时间：19:03:32]\n",
      "\n",
      "[状态：ok，尝试次数：2] - 50.00% - crawler_1 > 3.https://www.rockauto.com/en/parts/melling,30420S,timing+chain+&+component+kit,5756\n",
      "[剩余数量：0] - [当前时间：19:03:36]\n",
      "\n",
      "[状态：ok，尝试次数：2] - 50.00% - crawler_1 > 10.https://www.rockauto.com/en/parts/melling,30490S,timing+chain+&+component+kit,5756\n",
      "[剩余数量：0] - [当前时间：19:03:45]\n",
      "\n",
      "输出ing...\n",
      "\n",
      "总数量：10\n",
      "\n",
      "[状态：ok，尝试次数：1] - 100.00% - crawler_2 > 5.https://www.rockauto.com/en/parts/motorcraft,FC1024,fuel+tank+cap,5900\n",
      "[剩余数量：5] - [当前时间：19:03:46]\n",
      "\n",
      "[状态：ok，尝试次数：1] - 100.00% - crawler_2 > 2.https://www.rockauto.com/en/parts/motorcraft,FC1015,fuel+tank+cap,5900\n",
      "[剩余数量：4] - [当前时间：19:03:46]\n",
      "\n",
      "[状态：ok，尝试次数：1] - 100.00% - crawler_2 > 3.https://www.rockauto.com/en/parts/motorcraft,FC1018,fuel+tank+cap,5900\n",
      "[剩余数量：3] - [当前时间：19:03:47]\n",
      "\n",
      "[状态：ok，尝试次数：2] - 100.00% - crawler_2 > 1.https://www.rockauto.com/en/parts/motorcraft,FC1013,fuel+tank+cap,5900\n",
      "[剩余数量：2] - [当前时间：19:03:48]\n",
      "\n",
      "[状态：ok，尝试次数：1] - 100.00% - crawler_2 > 8.https://www.rockauto.com/en/parts/motorcraft,FC1054,fuel+tank+cap,5900\n",
      "[剩余数量：1] - [当前时间：19:03:49]\n",
      "\n",
      "[状态：ok，尝试次数：1] - 100.00% - crawler_2 > 7.https://www.rockauto.com/en/parts/motorcraft,FC1036,fuel+tank+cap,5900\n",
      "[剩余数量：0] - [当前时间：19:03:49]\n",
      "\n",
      "[状态：ok，尝试次数：2] - 100.00% - crawler_2 > 4.https://www.rockauto.com/en/parts/motorcraft,FC1021,fuel+tank+cap,5900\n",
      "[剩余数量：0] - [当前时间：19:03:49]\n",
      "\n",
      "[状态：ok，尝试次数：1] - 100.00% - crawler_2 > 9.https://www.rockauto.com/en/parts/motorcraft,FC1055,fuel+tank+cap,5900\n",
      "[剩余数量：0] - [当前时间：19:03:50]\n",
      "\n",
      "[状态：ok，尝试次数：2] - 100.00% - crawler_2 > 6.https://www.rockauto.com/en/parts/motorcraft,FC1032,fuel+tank+cap,5900\n",
      "[剩余数量：0] - [当前时间：19:03:50]\n",
      "\n",
      "[状态：ok，尝试次数：2] - 100.00% - crawler_2 > 10.https://www.rockauto.com/en/parts/motorcraft,FC1058,fuel+tank+cap,5900\n",
      "[剩余数量：0] - [当前时间：19:03:52]\n",
      "\n",
      "输出ing...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████████████████████████| 2/2 [00:00<00:00, 4341.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from gevent import monkey\n",
    "monkey.patch_all(thread=False)\n",
    "from gevent.queue import Queue\n",
    "import gevent\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "if os.path.exists('./file/2.part'):\n",
    "    shutil.rmtree('./file/2.part')\n",
    "\n",
    "os.makedirs('./file/2.part')\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import sys\n",
    "sys.path.append('../00.Tools')\n",
    "from crawler_configuration import get_header, get_proxy\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "import re\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "list_file = sorted(list(os.walk('./file/1.part_number'))[0][2])\n",
    "while '.DS_Store' in list_file:\n",
    "    list_file.remove('.DS_Store')\n",
    "while '0.null.txt' in list_file:\n",
    "    list_file.remove('0.null.txt')\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "for file in list_file:\n",
    "    \n",
    "    # = = = = = = = = = = = = = = =\n",
    "\n",
    "    output_correct, output_error = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # = = = = = = = = = = = = = = =\n",
    "\n",
    "    input_ = pd.read_excel(f'./file/1.part_number/{file}',\n",
    "                           header=0,\n",
    "                           dtype=str).fillna('')\n",
    "    \n",
    "    input_['No'] = input_['No'].astype(int)\n",
    "\n",
    "    print(f'总数量：{len(input_)}')\n",
    "    print()\n",
    "\n",
    "    # = = = = = = = = = = = = = = =\n",
    "\n",
    "\n",
    "    work = Queue()\n",
    "    for a in range(len(input_)):\n",
    "        work.put_nowait(a)\n",
    "\n",
    "    # = = = = = = = = = = = = = = =\n",
    "\n",
    "    def crawler():\n",
    "        global output_correct, output_error\n",
    "\n",
    "        while not work.empty():\n",
    "            a = work.get_nowait()\n",
    "\n",
    "            # = = = = = = = = = = = = = = =\n",
    "\n",
    "            crawler_status = 'error'\n",
    "\n",
    "            # = = = = = = = = = = = = = = =\n",
    "\n",
    "            try:\n",
    "                data = {'dopartsearch': '1',\n",
    "                        'partsearch[partnum][partsearch_007]': input_.loc[a, 'Part Number'],\n",
    "                        'partsearch[manufacturer][partsearch_007]': input_.loc[a, 'Manufacturer'],\n",
    "                        'partsearch[parttype][partsearch_007]': input_.loc[a, 'Type Code'],\n",
    "                        'partsearch[do][partsearch_007]': '1',\n",
    "                        'func': 'sendparttabsearch',\n",
    "                        'giveback': json.dumps({}),\n",
    "                        'api_json_request': '1'}\n",
    "\n",
    "                b = 0\n",
    "                while True:\n",
    "                    b += 1\n",
    "                    try:\n",
    "                        check_url = input_.loc[a, 'Url']\n",
    "                        \n",
    "                        resp = requests.post('https://www.rockauto.com/catalog/catalogapi.php',\n",
    "                                             data=data,\n",
    "                                             headers=get_header(),\n",
    "                                             proxies=get_proxy(),\n",
    "                                             timeout=(10, 10))\n",
    "\n",
    "                        if resp.status_code == 200:\n",
    "                            dict_ = resp.json()\n",
    "                            \n",
    "                            if 'redirect_to_url' not in dict_ and not dict_['giveback'] and 'collected_javascript' in dict_:\n",
    "                                break\n",
    "                    except KeyboardInterrupt:\n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "                            \n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                soup = BeautifulSoup(dict_['searchnoderesults'], 'lxml')\n",
    "                html = etree.HTML(str(soup))\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                list_tbody = html.xpath('//tbody[contains(@id, \"listingcontainer[__GIP__\")]')\n",
    "                \n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                list_list_note = [tbody.xpath('./tr[1]/td[contains(@class, \"listing-inner-content\")]/div[@class=\"listing-text-row-moreinfo-truck\"]/span[@class=\"span-link-underline-remover\"]') for tbody in list_tbody]\n",
    "                list_list_note = [[etree.tostring(note).decode('utf-8') for note in list_note] if list_note else [] for list_note in list_list_note]\n",
    "                list_list_note = [[re.sub(r'<.*?>', '', note) for note in list_note] for list_note in list_list_note]\n",
    "                list_list_note = [[re.sub(r' +', ' ', note).strip() for note in list_note] for list_note in list_list_note]\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                list_dict_note = []\n",
    "                for list_note in list_list_note:\n",
    "                    dict_note = {}\n",
    "                    for i, note in enumerate(list_note):\n",
    "                        dict_note[str(i)] = note\n",
    "                \n",
    "                    list_dict_note.append(dict_note)\n",
    "                \n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                list_list_note = [tbody.xpath('./tr[1]/td[contains(@class, \"listing-inner-content\")]/div[@class=\"listing-text-row\"]/span[contains(@title, \"Please make sure\")]') for tbody in list_tbody]\n",
    "                list_list_note = [[etree.tostring(text).decode('utf-8') for note in list_note for text in note.xpath('./span[@class=\"listing-footnote-text\"]')] if list_note else [] for list_note in list_list_note]\n",
    "                list_list_note = [[re.sub(r'<.*?>', '', note) for note in list_note] for list_note in list_list_note]\n",
    "                list_list_note = [[re.sub(r' +', ' ', note).strip() for note in list_note] for list_note in list_list_note]\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                list_dict_note_2 = []\n",
    "                for list_note in list_list_note:\n",
    "                    dict_note = {}\n",
    "                    for i, note in enumerate(list_note):\n",
    "                        dict_note[str(i)] = note\n",
    "                \n",
    "                    list_dict_note_2.append(dict_note)\n",
    "    \n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                list_dict = [json.loads(tbody.xpath('./tr[1]/td[contains(@class, \"listing-inner-content\")]/input[contains(@id, \"listing_data_essential[__GIP__\")]/@value')[0]) for tbody in list_tbody]\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                list_type_code = [dict_['parttype'] for dict_ in list_dict]\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                array_index_type_code = np.where(np.array(list_type_code) == input_.loc[a, 'Type Code'])\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                list_part_code = [dict_['partkey'] for dict_ in list_dict]\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                list_list_info = [tbody.xpath('./tr[1]/td[contains(@class, \"listing-inner-content\")]/div[@class=\"listing-text-row-moreinfo-truck\"]/a[@class=\"ra-btn ra-btn-moreinfo\"]') for tbody in list_tbody]\n",
    "                list_info = [list_info[0].xpath('./@href')[0].strip() if list_info else '' for list_info in list_list_info]\n",
    "                list_info = [info if info.startswith('https://www.rockauto.com/en/moreinfo.php?') else f'''https://www.rockauto.com/en/moreinfo.php?pk={part_code}&cc=0&pt={type_code}&Lennon=1''' for info, part_code, type_code in zip(list_info, list_part_code, list_type_code)]\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                list_dict = [json.loads(tbody.xpath('./tr[1]/td/input[contains(@id, \"listing_data_supplemental[__GIP__\")]/@value')[0]) for tbody in list_tbody]\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                array_index_part_number = np.where(np.array([dict_['partnumber'] for dict_ in list_dict]) == input_.loc[a, 'Part Number'])\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                array_index_manufacturer = np.where(np.array([dict_['catalogname'] for dict_ in list_dict]) == input_.loc[a, 'Manufacturer'])\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                array_index_part = np.intersect1d(np.intersect1d(array_index_type_code, array_index_part_number), array_index_manufacturer)\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                if len(array_index_part) == 1:\n",
    "                    index_part = array_index_part[0]\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                choose = list_dict[index_part]['paramdesc'].strip()\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                list_list_text = [td.xpath('./span[contains(@title, \"Replaces these Alternate\")]/text()') for td in list_tbody[index_part].xpath('./tr[1]/td[contains(@class, \"listing-inner-content\")]')]\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                list_oem = []\n",
    "                for list_text in list_list_text:\n",
    "                    if list_text:\n",
    "                        for text in list_text:\n",
    "                            list_oem += text.split(',')\n",
    "                        \n",
    "                list_oem = [oem.strip() for oem in list_oem]\n",
    "                    \n",
    "                while '' in list_oem:\n",
    "                    list_oem.remove('')\n",
    "                \n",
    "                oem = ';'.join(sorted(list(set(list_oem))))\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                list_src = list_tbody[index_part].xpath('./tr[1]/td/div[contains(@class, \"listing-image-space-occupy\")]/input[contains(@id, \"jsninlineimg[__GIP__\")]')\n",
    "                dict_ = json.loads(list_src[0].xpath('./@value')[0]) if list_src else {}\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                dict_src = {}\n",
    "                if dict_:\n",
    "                    list_src = dict_['Slots']\n",
    "                    for i, src in enumerate(list_src):\n",
    "                        dict_src[str(i)] = f'''https://www.rockauto.com{src['ImageData']['Full'].strip()}'''\n",
    "        \n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                price = list_tbody[index_part].xpath('./tr[1]/td[contains(@id, \"listingtd[__GIP__\") and contains(@id, \"__][price]\")]/span[contains(@id, \"dprice[__GIP__\") and contains(@id, \"__][td]\")]/span/span[contains(@id, \"dprice[__GIP__\") and contains(@id, \"__][v]\")]')\n",
    "                price = etree.tostring(price[0]).decode('utf-8') if price else ''\n",
    "                price = re.sub(r'<.*?>', '', price)\n",
    "                price = re.sub(r' +', ' ', price).strip()\n",
    "                dict_price = {'0': price}\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                td = list_tbody[index_part].xpath('./tr[1]/td[contains(@class, \"listing-inner-content\")]')[0]\n",
    "                list_option = [etree.tostring(option).decode('utf-8') for option in td.xpath('./div/div[contains(@class, \"listing-optionchoice-multiple-container\")]/span[contains(@id, \"ddrepl[optionchoice[__GIP__\") and contains(@id, \"__]][container]\")]/span[@class=\"hide-if-js\"]/select[contains(@id, \"optionchoice[__GIP__\") and contains(@id, \"__]\")]/option[@value!=\"\"]')]\n",
    "                list_option = [re.sub(r'<.*?>', '', option) for option in list_option]\n",
    "                list_option = [re.sub(r' +', ' ', option).strip() for option in list_option]\n",
    "                if list_option:\n",
    "                    dict_price['1'] = {}\n",
    "                    for i, option in enumerate(list_option):\n",
    "                        dict_price['1'][str(i)] = option\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                df_temp = pd.DataFrame([{'No': input_.loc[a, 'No'],\n",
    "                                         'Type': input_.loc[a, 'Type'],\n",
    "                                         'Manufacturer': input_.loc[a, 'Manufacturer'],\n",
    "                                         'Part Number': input_.loc[a, 'Part Number'],\n",
    "                                         'Vehicle': '',\n",
    "                                         'OEM': oem,\n",
    "                                         'Picture': '',\n",
    "                                         'Url': input_.loc[a, 'Url'],\n",
    "                                         'Json_Src': json.dumps(dict_src),\n",
    "                                         'Info': list_info[index_part],\n",
    "                                         'Choose': choose,\n",
    "                                         'Json_Price': json.dumps(dict_price),\n",
    "                                         'Json_Note 1': json.dumps(list_dict_note[index_part]),\n",
    "                                         'Json_Note 2': json.dumps(list_dict_note_2[index_part]),\n",
    "                                         'Group': input_.loc[a, 'Group'],\n",
    "                                         'Type Code': input_.loc[a, 'Type Code'],\n",
    "                                         'Part Code': list_part_code[index_part],\n",
    "                                         'JOIN_MPNTCPC': f'''{input_.loc[a, 'Manufacturer']};{input_.loc[a, 'Part Number']};{input_.loc[a, 'Type Code']};{list_part_code[index_part]}'''}])\n",
    "\n",
    "                output_correct = pd.concat([output_correct, df_temp], ignore_index=True).fillna('')\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                crawler_status = 'ok'\n",
    "            except:\n",
    "                df_temp = pd.DataFrame([input_.iloc[a]]).reset_index(drop=True)\n",
    "                df_temp.loc[0, 'Check_Url'] = check_url\n",
    "                \n",
    "                output_error = pd.concat([output_error, df_temp], ignore_index=True).fillna('')\n",
    "\n",
    "            # = = = = = = = = = = = = = = =\n",
    "\n",
    "            if list_file.index(file) % 3 == 0 and list_file.index(file) != 0 and work.qsize() == 0 or work.qsize() % 1_000 == 0 and work.qsize() != 0:\n",
    "                clear_output()\n",
    "\n",
    "            print(f'''[状态：{crawler_status}，尝试次数：{b}] - {(list_file.index(file) + 1) / len(list_file) * 100:.2f}% - {file.removesuffix('.xlsx')} > {input_.loc[a, 'No']}.{check_url}\\n[剩余数量：{work.qsize()}] - [当前时间：{datetime.now().strftime('%H:%M:%S')}]\\n''')\n",
    "\n",
    "    # = = = = = = = = = = = = = = =\n",
    "\n",
    "    list_task = []\n",
    "    for _ in range(5):\n",
    "        task = gevent.spawn(crawler)\n",
    "        list_task.append(task)\n",
    "    gevent.joinall(list_task)\n",
    "\n",
    "    print('输出ing...')\n",
    "    print()\n",
    "    if not output_correct.empty:\n",
    "        output_correct = output_correct.sort_values(by=['No'],\n",
    "                                                    ascending=[True],\n",
    "                                                    ignore_index=True)\n",
    "        output_correct.to_excel(f'''./file/2.part/{file.removesuffix('.xlsx')}-{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx''', index=False)\n",
    "    if not output_error.empty:\n",
    "        output_error = output_error.sort_values(by=['No'],\n",
    "                                                ascending=[True],\n",
    "                                                ignore_index=True)\n",
    "        output_error.to_excel(f'''./file/{file.removesuffix('.xlsx')}-part_error.xlsx''', index=False)\n",
    "        print('爬虫存在error')\n",
    "        print()\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "list_file = sorted(list(os.walk('./file/2.part'))[0][2])\n",
    "while '.DS_Store' in list_file:\n",
    "    list_file.remove('.DS_Store')\n",
    "while '0.null.txt' in list_file:\n",
    "    list_file.remove('0.null.txt')\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for file in tqdm(list_file, desc='Progress', ncols=77):\n",
    "    os.rename(f'./file/2.part/{file}',\n",
    "              f'''./file/2.part/{re.sub(r'-[0-9]{8}_[0-9]{6}.xlsx$', '.xlsx', file)}''')\n",
    "\n",
    "print('Done ~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b85824-3eb7-4c9b-b273-da652c66cd61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
