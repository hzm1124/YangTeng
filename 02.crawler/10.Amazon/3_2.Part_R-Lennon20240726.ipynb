{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b786d7ed-adec-45a0-a457-744c96ab0d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gevent import monkey\n",
    "monkey.patch_all(thread=False)\n",
    "from gevent.queue import Queue\n",
    "import gevent\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7916ee44-7f15-4de6-927a-51391c1f385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_session_id = '137-5565607-0668155'\n",
    "param_ubid_main = '135-9546146-6073418'\n",
    "param_session_token = '3iuUPMVP9pez3dDPQbNug4OUnEGlaHZeQY9Lce8uZBCsPt4SKBcnO64MyZyzMa2RnnwQ1uME9HxpBWntNnbtQzSwlsoNuCEiRDccVzgX92EuNgQ3/11zpOlsqJdSYsBORGoLYBF62oKjF9owTY858bDLhii5koaRu+QGJ8WJUudg+Oaa3LDFmWWcmKNoUy8eYXv58AFaGQwBIx38AHJXAS8G5Jso/PCkh6HPeWeqBi1EKov9b1Mh/pST5ekzars+FQlSa58bO9iDhOSy4v9htikWD2im1H0XywbNJQNRFwoHGkM2Y5fWjV7B8CBcaWcHQcCQUNh5WUnWX9C0w5dZ4UGvdLucCbUC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1463dd92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总数量：100\n",
      "\n",
      "[状态：ok，尝试次数：1] - 2.B009B5S5E8\n",
      "[剩余数量：88] - [当前时间：19:24:36]\n",
      "\n",
      "[状态：ok，尝试次数：1] - 3.B0CN8J4K53\n",
      "[剩余数量：87] - [当前时间：19:24:36]\n",
      "\n",
      "[状态：ok，尝试次数：18] - 7.B0CYYYL59H\n",
      "[剩余数量：86] - [当前时间：19:24:37]\n",
      "\n",
      "[状态：ok，尝试次数：23] - 1.B08W98H7H7\n",
      "[剩余数量：85] - [当前时间：19:24:38]\n",
      "\n",
      "[状态：ok，尝试次数：9] - 15.B00OU7PZCM\n",
      "[剩余数量：84] - [当前时间：19:24:42]\n",
      "\n",
      "[状态：ok，尝试次数：12] - 10.B09PH28FG2\n",
      "[剩余数量：83] - [当前时间：19:24:48]\n",
      "\n",
      "[状态：ok，尝试次数：42] - 8.B009TKMXV6\n",
      "[剩余数量：82] - [当前时间：19:24:49]\n",
      "\n",
      "[状态：ok，尝试次数：112] - 6.B08TTC1HQ3\n",
      "[剩余数量：81] - [当前时间：19:24:49]\n",
      "\n",
      "[状态：ok，尝试次数：29] - 11.B0BF9SR4VN\n",
      "[剩余数量：80] - [当前时间：19:24:50]\n",
      "\n",
      "[状态：ok，尝试次数：48] - 9.B08W9MX59Y\n",
      "[剩余数量：79] - [当前时间：19:24:52]\n",
      "\n",
      "[状态：ok，尝试次数：36] - 5.B0CW2SKD6R\n",
      "[剩余数量：78] - [当前时间：19:24:53]\n",
      "\n",
      "[状态：ok，尝试次数：116] - 14.B0BPCP4XQN\n",
      "[剩余数量：77] - [当前时间：19:24:55]\n",
      "\n",
      "[状态：ok，尝试次数：91] - 17.B0BY213GX3\n",
      "[剩余数量：76] - [当前时间：19:24:58]\n",
      "\n",
      "[状态：ok，尝试次数：49] - 18.B01KHL31PW\n",
      "[剩余数量：75] - [当前时间：19:25:00]\n",
      "\n",
      "[状态：ok，尝试次数：22] - 22.B097JWWR4V\n",
      "[剩余数量：74] - [当前时间：19:25:00]\n",
      "\n",
      "[状态：ok，尝试次数：19] - 19.B09V88967X\n",
      "[剩余数量：73] - [当前时间：19:25:01]\n",
      "\n",
      "[状态：ok，尝试次数：47] - 13.B00M370TI6\n",
      "[剩余数量：72] - [当前时间：19:25:04]\n",
      "\n",
      "[状态：ok，尝试次数：5] - 24.B0D5847ZLT\n",
      "[剩余数量：71] - [当前时间：19:25:05]\n",
      "\n",
      "[状态：ok，尝试次数：71] - 16.B0963B61HD\n",
      "[剩余数量：70] - [当前时间：19:25:05]\n",
      "\n",
      "[状态：ok，尝试次数：22] - 28.B004SIXYXU\n",
      "[剩余数量：69] - [当前时间：19:25:06]\n",
      "\n",
      "[状态：ok，尝试次数：93] - 4.B004SJN66O\n",
      "[剩余数量：68] - [当前时间：19:25:07]\n",
      "\n",
      "[状态：ok，尝试次数：36] - 25.B01DQ1B0ZS\n",
      "[剩余数量：67] - [当前时间：19:25:07]\n",
      "\n",
      "[状态：ok，尝试次数：19] - 30.B0BN3HY23Y\n",
      "[剩余数量：66] - [当前时间：19:25:11]\n",
      "\n",
      "[状态：ok，尝试次数：11] - 33.B004A2D6DM\n",
      "[剩余数量：65] - [当前时间：19:25:11]\n",
      "\n",
      "[状态：ok，尝试次数：9] - 26.B0C7WDPGV7\n",
      "[剩余数量：64] - [当前时间：19:25:14]\n",
      "\n",
      "[状态：ok，尝试次数：20] - 31.B006EFV6KS\n",
      "[剩余数量：63] - [当前时间：19:25:20]\n",
      "\n",
      "[状态：ok，尝试次数：72] - 36.B0BYXR6L1R\n",
      "[剩余数量：62] - [当前时间：19:25:27]\n",
      "\n",
      "[状态：ok，尝试次数：111] - 32.B0CRHCPWQ9\n",
      "[剩余数量：61] - [当前时间：19:25:28]\n",
      "\n",
      "[状态：ok，尝试次数：123] - 23.B0CZ6TQ86D\n",
      "[剩余数量：60] - [当前时间：19:25:29]\n",
      "\n",
      "[状态：ok，尝试次数：14] - 35.B0C4HTBXDL\n",
      "[剩余数量：59] - [当前时间：19:25:30]\n",
      "\n",
      "[状态：ok，尝试次数：97] - 34.B0CW99T1MR\n",
      "[剩余数量：58] - [当前时间：19:25:30]\n",
      "\n",
      "[状态：ok，尝试次数：41] - 12.B0C81FVYS9\n",
      "[剩余数量：57] - [当前时间：19:25:31]\n",
      "\n",
      "[状态：ok，尝试次数：7] - 43.B089YFSSL1\n",
      "[剩余数量：56] - [当前时间：19:25:35]\n",
      "\n",
      "[状态：ok，尝试次数：28] - 41.B000C5C4GG\n",
      "[剩余数量：55] - [当前时间：19:25:36]\n",
      "\n",
      "[状态：ok，尝试次数：5] - 42.B0CDPXRFWQ\n",
      "[剩余数量：54] - [当前时间：19:25:36]\n",
      "\n",
      "[状态：ok，尝试次数：34] - 29.B010GO6EAE\n",
      "[剩余数量：53] - [当前时间：19:25:44]\n",
      "\n",
      "[状态：ok，尝试次数：60] - 21.B076PRPYRS\n",
      "[剩余数量：52] - [当前时间：19:25:45]\n",
      "\n",
      "[状态：ok，尝试次数：60] - 40.B01E6G23MG\n",
      "[剩余数量：51] - [当前时间：19:25:45]\n",
      "\n",
      "[状态：ok，尝试次数：24] - 27.B093Q6SV5Z\n",
      "[剩余数量：50] - [当前时间：19:25:46]\n",
      "\n",
      "[状态：ok，尝试次数：46] - 47.B00809W1M8\n",
      "[剩余数量：49] - [当前时间：19:25:46]\n",
      "\n",
      "[状态：ok，尝试次数：12] - 51.B0D2V9CFXN\n",
      "[剩余数量：48] - [当前时间：19:25:51]\n",
      "\n",
      "[状态：ok，尝试次数：24] - 38.B0D7927R5J\n",
      "[剩余数量：47] - [当前时间：19:25:54]\n",
      "\n",
      "[状态：ok，尝试次数：56] - 20.B0BBY8VHG7\n",
      "[剩余数量：46] - [当前时间：19:25:55]\n",
      "\n",
      "[状态：ok，尝试次数：110] - 39.B0CHRZFQNV\n",
      "[剩余数量：45] - [当前时间：19:25:56]\n",
      "\n",
      "[状态：ok，尝试次数：10] - 55.B0CYWNVSNG\n",
      "[剩余数量：44] - [当前时间：19:26:00]\n",
      "\n",
      "[状态：ok，尝试次数：117] - 49.B09H5JQKQJ\n",
      "[剩余数量：43] - [当前时间：19:26:02]\n",
      "\n",
      "[状态：ok，尝试次数：64] - 48.B0CZ33S81S\n",
      "[剩余数量：42] - [当前时间：19:26:02]\n",
      "\n",
      "[状态：ok，尝试次数：68] - 44.B093S8XVSL\n",
      "[剩余数量：41] - [当前时间：19:26:02]\n",
      "\n",
      "[状态：ok，尝试次数：2] - 57.B0BHHV42TQ\n",
      "[剩余数量：40] - [当前时间：19:26:04]\n",
      "\n",
      "[状态：ok，尝试次数：1] - 46.B09YD2YJ6M\n",
      "[剩余数量：39] - [当前时间：19:26:06]\n",
      "\n",
      "[状态：ok，尝试次数：61] - 54.B07KFKRM26\n",
      "[剩余数量：38] - [当前时间：19:26:07]\n",
      "\n",
      "[状态：ok，尝试次数：4] - 63.B08NVLBNK8\n",
      "[剩余数量：37] - [当前时间：19:26:11]\n",
      "\n",
      "[状态：ok，尝试次数：127] - 37.B0C61D28WR\n",
      "[剩余数量：36] - [当前时间：19:26:12]\n",
      "\n",
      "[状态：ok，尝试次数：119] - 45.B0C7Z4SGZW\n",
      "[剩余数量：35] - [当前时间：19:26:13]\n",
      "\n",
      "[状态：ok，尝试次数：5] - 61.B00HQ2CBNE\n",
      "[剩余数量：34] - [当前时间：19:26:17]\n",
      "\n",
      "[状态：ok，尝试次数：38] - 59.B09F34ZCQJ\n",
      "[剩余数量：33] - [当前时间：19:26:19]\n",
      "\n",
      "[状态：ok，尝试次数：25] - 65.B0C5ZHH3WQ\n",
      "[剩余数量：32] - [当前时间：19:26:21]\n",
      "\n",
      "[状态：ok，尝试次数：32] - 66.B00809W0YC\n",
      "[剩余数量：31] - [当前时间：19:26:23]\n",
      "\n",
      "[状态：ok，尝试次数：16] - 67.B09TB2SSNB\n",
      "[剩余数量：30] - [当前时间：19:26:28]\n",
      "\n",
      "[状态：ok，尝试次数：85] - 50.B0CXPJJMW4\n",
      "[剩余数量：29] - [当前时间：19:26:31]\n",
      "\n",
      "[状态：ok，尝试次数：16] - 71.B0C3ML1NDC\n",
      "[剩余数量：28] - [当前时间：19:26:33]\n",
      "\n",
      "[状态：ok，尝试次数：62] - 52.B0CC8P4YLL\n",
      "[剩余数量：27] - [当前时间：19:26:33]\n",
      "\n",
      "[状态：ok，尝试次数：131] - 53.B0CJF4J9N1\n",
      "[剩余数量：26] - [当前时间：19:26:37]\n",
      "\n",
      "[状态：ok，尝试次数：7] - 64.B09PH13WTM\n",
      "[剩余数量：25] - [当前时间：19:26:37]\n",
      "\n",
      "[状态：ok，尝试次数：60] - 58.B0CBM9N9B2\n",
      "[剩余数量：24] - [当前时间：19:26:41]\n",
      "\n",
      "[状态：ok，尝试次数：11] - 75.B0C5Z9TM23\n",
      "[剩余数量：23] - [当前时间：19:26:42]\n",
      "\n",
      "[状态：ok，尝试次数：31] - 72.B09Y61GJD1\n",
      "[剩余数量：22] - [当前时间：19:26:46]\n",
      "\n",
      "[状态：ok，尝试次数：74] - 60.B0BZPJ6KDK\n",
      "[剩余数量：21] - [当前时间：19:26:47]\n",
      "\n",
      "[状态：ok，尝试次数：4] - 79.B0CG7LMX28\n",
      "[剩余数量：20] - [当前时间：19:26:50]\n",
      "\n",
      "[状态：ok，尝试次数：50] - 78.B09J7PZLPH\n",
      "[剩余数量：19] - [当前时间：19:26:50]\n",
      "\n",
      "[状态：ok，尝试次数：60] - 68.B0BWJNZCMZ\n",
      "[剩余数量：18] - [当前时间：19:26:51]\n",
      "\n",
      "[状态：ok，尝试次数：26] - 74.B00HQ2CCCE\n",
      "[剩余数量：17] - [当前时间：19:26:51]\n",
      "\n",
      "[状态：ok，尝试次数：35] - 77.B0CDP8NCMJ\n",
      "[剩余数量：16] - [当前时间：19:26:52]\n",
      "\n",
      "[状态：ok，尝试次数：7] - 81.B09KR47HS3\n",
      "[剩余数量：15] - [当前时间：19:26:55]\n",
      "\n",
      "[状态：ok，尝试次数：38] - 76.B00HQ2C6KM\n",
      "[剩余数量：14] - [当前时间：19:26:55]\n",
      "\n",
      "[状态：ok，尝试次数：100] - 56.B09P54QWQ8\n",
      "[剩余数量：13] - [当前时间：19:26:58]\n",
      "\n",
      "[状态：ok，尝试次数：13] - 85.B0BR4WKW3H\n",
      "[剩余数量：12] - [当前时间：19:26:58]\n",
      "\n",
      "[状态：ok，尝试次数：1] - 87.B0CL91HCJF\n",
      "[剩余数量：11] - [当前时间：19:26:59]\n",
      "\n",
      "[状态：ok，尝试次数：13] - 86.B0B179GW1C\n",
      "[剩余数量：10] - [当前时间：19:27:01]\n",
      "\n",
      "[状态：ok，尝试次数：9] - 89.B0CH5NL9MN\n",
      "[剩余数量：9] - [当前时间：19:27:03]\n",
      "\n",
      "[状态：ok，尝试次数：22] - 83.B0CYLYQ8M4\n",
      "[剩余数量：8] - [当前时间：19:27:05]\n",
      "\n",
      "[状态：ok，尝试次数：10] - 91.B00JIWAX12\n",
      "[剩余数量：7] - [当前时间：19:27:08]\n",
      "\n",
      "[状态：ok，尝试次数：16] - 92.B0BZ19PYGS\n",
      "[剩余数量：6] - [当前时间：19:27:08]\n",
      "\n",
      "[状态：ok，尝试次数：15] - 84.B0BH4ZXNKP\n",
      "[剩余数量：5] - [当前时间：19:27:10]\n",
      "\n",
      "[状态：ok，尝试次数：22] - 93.B095C2ZSK1\n",
      "[剩余数量：4] - [当前时间：19:27:11]\n",
      "\n",
      "[状态：ok，尝试次数：5] - 95.B09WJQZDRN\n",
      "[剩余数量：3] - [当前时间：19:27:12]\n",
      "\n",
      "[状态：ok，尝试次数：102] - 70.B09KN6WKQ9\n",
      "[剩余数量：2] - [当前时间：19:27:13]\n",
      "\n",
      "[状态：ok，尝试次数：73] - 69.B092PYD7P7\n",
      "[剩余数量：1] - [当前时间：19:27:18]\n",
      "\n",
      "[状态：ok，尝试次数：12] - 100.B0945T4JB3\n",
      "[剩余数量：0] - [当前时间：19:27:23]\n",
      "\n",
      "[状态：ok，尝试次数：9] - 94.B0C6SWXZVX\n",
      "[剩余数量：0] - [当前时间：19:27:23]\n",
      "\n",
      "[状态：ok，尝试次数：14] - 80.B0CNJZ51QG\n",
      "[剩余数量：0] - [当前时间：19:27:28]\n",
      "\n",
      "[状态：ok，尝试次数：31] - 62.B01G48AJ4S\n",
      "[剩余数量：0] - [当前时间：19:27:35]\n",
      "\n",
      "[状态：ok，尝试次数：13] - 97.B004SJ02SE\n",
      "[剩余数量：0] - [当前时间：19:27:40]\n",
      "\n",
      "[状态：ok，尝试次数：47] - 82.B07FZ7ZHXR\n",
      "[剩余数量：0] - [当前时间：19:27:41]\n",
      "\n",
      "[状态：ok，尝试次数：79] - 73.B01K8IX2OO\n",
      "[剩余数量：0] - [当前时间：19:27:43]\n",
      "\n",
      "[状态：ok，尝试次数：110] - 96.B008B68P3O\n",
      "[剩余数量：0] - [当前时间：19:27:44]\n",
      "\n",
      "[状态：ok，尝试次数：153] - 88.B0CQ1P4B58\n",
      "[剩余数量：0] - [当前时间：19:27:44]\n",
      "\n",
      "[状态：ok，尝试次数：189] - 90.B07PGLCTJ6\n",
      "[剩余数量：0] - [当前时间：19:27:51]\n",
      "\n",
      "[状态：ok，尝试次数：129] - 99.B095SB3KWW\n",
      "[剩余数量：0] - [当前时间：19:28:09]\n",
      "\n",
      "[状态：ok，尝试次数：18] - 98.B0BPM39KYM\n",
      "[剩余数量：0] - [当前时间：19:28:32]\n",
      "\n",
      "输出ing...\n",
      "\n",
      "Done ~\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "output_correct, output_error = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "input_ = pd.read_excel('./2.menu.xlsx',\n",
    "                       header=0,\n",
    "                       dtype=str).fillna('')\n",
    "\n",
    "input_['No'] = input_['No'].astype(int)\n",
    "\n",
    "print(f'总数量：{len(input_)}')\n",
    "print()\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "work = Queue()\n",
    "for a in range(len(input_)):\n",
    "    work.put_nowait(a)\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "def crawler():\n",
    "    global output_correct, output_error\n",
    "\n",
    "    while not work.empty():\n",
    "        a = work.get_nowait()\n",
    "\n",
    "        # = = = = = = = = = = = = = = =\n",
    "\n",
    "        crawler_status = 'error'\n",
    "\n",
    "        # = = = = = = = = = = = = = = =\n",
    "\n",
    "        import requests\n",
    "\n",
    "        import sys\n",
    "        sys.path.append('../00.Tools')\n",
    "        from crawler_configuration import get_header, get_proxy\n",
    "\n",
    "        from bs4 import BeautifulSoup\n",
    "        from lxml import etree\n",
    "\n",
    "        import re\n",
    "\n",
    "        import json\n",
    "\n",
    "        if input_.loc[a, 'Market'] == 'US':\n",
    "            url = f'''https://www.amazon.com/dp/{input_.loc[a, 'ASIN']}?language=en_US'''\n",
    "\n",
    "        cookie = {'session-id': param_session_id,\n",
    "                  'ubid-main': param_ubid_main,\n",
    "                  'session-token': param_session_token}\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                \n",
    "                b = 0\n",
    "                while True:\n",
    "\n",
    "                    while True:\n",
    "                        b += 1\n",
    "                        try:\n",
    "                            resp = requests.get(url,\n",
    "                                                headers=get_header(),\n",
    "                                                proxies=get_proxy(),\n",
    "                                                cookies=cookie,\n",
    "                                                timeout=(10, 10))\n",
    "                            break\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "                    if resp.status_code == 200:\n",
    "                        soup = BeautifulSoup(resp.text, 'lxml')\n",
    "                        html = etree.HTML(str(soup))\n",
    "\n",
    "                        # = = = = = = = = = = = = = = =\n",
    "\n",
    "                        list_captcha = html.xpath('//div[@class=\"a-box-inner\"]/h4/text()')\n",
    "\n",
    "                        # = = = = = = = = = = = = = = =\n",
    "\n",
    "                        if not list_captcha or list_captcha[0].strip() != 'Enter the characters you see below':\n",
    "                            break\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                zip_code = re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', html.xpath('//span[@id=\"glow-ingress-line2\"]/text()')[0]).strip()\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                price = html.xpath('//span[@class=\"a-offscreen\"]/text()')[0].strip()\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                json_ = json.loads('{\"initial\":[{' + resp.text.split('\\'colorImages\\': { \\'initial\\': [{')[1].split('}]},')[0] + '}]}')\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                dict_src = {}\n",
    "                list_src = [src['hiRes'].strip() if src['hiRes'] else src['large'].strip() for src in json_['initial']]\n",
    "                for i, src in zip(range(len(list_src)), list_src):\n",
    "                    dict_src[str(i)] = src\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                dict_overview = {}\n",
    "                list_tr = html.xpath('//table[@class=\"a-normal a-spacing-micro\"]/tr')\n",
    "                for i, tr in zip(range(len(list_tr)), list_tr):\n",
    "                    list_span = [re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', text).strip() for text in tr.xpath('./td/span/text()')]\n",
    "                    dict_overview[str(i)] = {list_span[0]: list_span[1]}\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                about = ''\n",
    "                list_span = [re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', text).strip() for text in html.xpath('//ul[@class=\"a-unordered-list a-vertical a-spacing-mini\"]/li/span/text()')]\n",
    "                for i, span in zip(range(len(list_span)), list_span):\n",
    "                    about += f'{i+1}. {span}\\n'\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                json_ = json.loads(resp.text.split('jQuery.parseJSON(\\'')[1].split('}\\');')[0] + '}')\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                title = re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', json_['title']).strip()\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                parent_asin = json_['parentAsin'].strip()\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                dict_detail = {}\n",
    "                list_tr = html.xpath('//table[@id=\"productDetails_techSpec_section_1\"]/tr')\n",
    "                for i, tr in zip(range(len(list_tr)), list_tr):\n",
    "                    dict_detail[str(i)] = {re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', tr.xpath('./th/text()')[0]).strip(): re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', tr.xpath('./td/text()')[0]).strip()}\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                dict_information = {}\n",
    "                list_tr = html.xpath('//table[@id=\"productDetails_detailBullets_sections1\"]/tr')\n",
    "                for i, tr in zip(range(len(list_tr)), list_tr):\n",
    "                    th = re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', tr.xpath('./th/text()')[0]).strip()\n",
    "                    if th == 'Customer Reviews':\n",
    "                        dict_information[str(i)] = {th: {}}\n",
    "                        dict_information[str(i)][th]['0'] = tr.xpath('./td/div/span[@data-action=\"acrStarsLink-click-metrics\"]/span/@title')[0].strip()\n",
    "                        dict_information[str(i)][th]['1'] = tr.xpath('./td/div/span[@data-action=\"acrLink-click-metrics\"]/a/span/text()')[0].strip()\n",
    "                    elif th == 'Best Sellers Rank':\n",
    "                        dict_information[str(i)] = {th: {}}\n",
    "                        list_span = [etree.tostring(span).decode('utf-8') for span in tr.xpath('./td/span/span')]\n",
    "                        list_span = [re.sub(r'<.*?>|\\(.*?\\)', '', span).strip() for span in list_span]\n",
    "                        list_href = [f'https://www.amazon.com{href.strip()}' for href in tr.xpath('./td/span/span/a/@href')]\n",
    "                        for j, span, href in zip(range(len(list_span)), list_span, list_href):\n",
    "                            dict_information[str(i)][th][str(j)] = {span: href}\n",
    "                    else:\n",
    "                        dict_information[str(i)] = {th: re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', tr.xpath('./td/text()')[0]).strip()}\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                dict_detail_2 = {}\n",
    "                list_span = html.xpath('//div[@id=\"detailBulletsWrapper_feature_div\"]/div[@id=\"detailBullets_feature_div\"]/ul/li/span')\n",
    "                for i, span in zip(range(len(list_span)), list_span):\n",
    "                    key = re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', span.xpath('./span/text()')[0]).strip()\n",
    "                    dict_detail_2[str(i)] = {key[:-1].strip() if key.endswith(':') else key: re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', span.xpath('./span/text()')[1]).strip()}\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                dict_information_2 = {}\n",
    "                list_span = html.xpath('//div[@id=\"detailBulletsWrapper_feature_div\"]/ul[@class=\"a-unordered-list a-nostyle a-vertical a-spacing-none detail-bullet-list\"]/li/span')\n",
    "                for i, span in zip(range(len(list_span)), list_span):\n",
    "                    key = re.sub(r'[\\u0020\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u200b\\u200c\\u200d\\u200e\\u200f\\u202f\\u205f\\u2060\\u2061\\u2062\\u2063\\u2064\\u3000\\ufeff ]+', ' ', span.xpath('./span/text()')[0]).strip()\n",
    "                    key = key[:-1].strip() if key.endswith(':') else key\n",
    "                    if key == 'Customer Reviews':\n",
    "                        dict_information_2[str(i)] = {key: {}}\n",
    "                        dict_information_2[str(i)][key]['0'] = span.xpath('./div/span[@data-action=\"acrStarsLink-click-metrics\"]/span/@title')[0].strip()\n",
    "                        dict_information_2[str(i)][key]['1'] = span.xpath('./div/span[@data-action=\"acrLink-click-metrics\"]/a/span/text()')[0].strip()\n",
    "                    elif key == 'Best Sellers Rank':\n",
    "                        dict_information_2[str(i)] = {key: {}}\n",
    "                        list_span_2 = [etree.tostring(span_2).decode('utf-8') for span_2 in span.xpath('./descendant-or-self::span[@class=\"a-list-item\"]')]\n",
    "                        list_span_2 = [re.sub(r'<ul.*?</ul>|<span.*?Best Sellers Rank.*?</span>|<.*?>|\\(.*?\\)', '', span_2).strip() for span_2 in list_span_2]\n",
    "                        list_href = [f'https://www.amazon.com{href.strip()}' for href in span.xpath('./descendant-or-self::a/@href')]\n",
    "                        for j, span_2, href in zip(range(len(list_span_2)), list_span_2, list_href):\n",
    "                            dict_information_2[str(i)][key][str(j)] = {span_2: href}\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                if not dict_detail and not dict_detail_2 and not dict_information and not dict_information_2:\n",
    "                    continue\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                df_temp = pd.DataFrame([{'No': input_.loc[a, 'No'],\n",
    "                                         'ASIN': input_.loc[a, 'ASIN'],\n",
    "                                         'Parent ASIN': parent_asin,\n",
    "                                         'Title': title,\n",
    "                                         'ZIP Code': zip_code,\n",
    "                                         'Price': price,\n",
    "                                         'About': about.strip(),\n",
    "                                         'Picture': '',\n",
    "                                         'Url': url,\n",
    "                                         'Json_Src': json.dumps(dict_src),\n",
    "                                         'Json_Overview': json.dumps(dict_overview),\n",
    "                                         'Json_Detail 1': json.dumps(dict_detail),\n",
    "                                         'Json_Detail 2': json.dumps(dict_detail_2),\n",
    "                                         'Json_Information 1': json.dumps(dict_information),\n",
    "                                         'Json_Information 2': json.dumps(dict_information_2)}])\n",
    "\n",
    "                output_correct = pd.concat([output_correct, df_temp], ignore_index=True).fillna('')\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                crawler_status = 'ok'\n",
    "\n",
    "                break\n",
    "        except:\n",
    "            df_temp = pd.DataFrame([{'Market': input_.loc[a, 'Market'],\n",
    "                                     'Page': input_.loc[a, 'Page'],\n",
    "                                     'Page Url': input_.loc[a, 'Page Url'],\n",
    "                                     'No': input_.loc[a, 'No'],\n",
    "                                     'ASIN': input_.loc[a, 'ASIN']}])\n",
    "\n",
    "            output_error = pd.concat([output_error, df_temp], ignore_index=True).fillna('')\n",
    "\n",
    "        # = = = = = = = = = = = = = = =\n",
    "\n",
    "        if work.qsize() % 1_000 == 0 and work.qsize() != 0:\n",
    "            clear_output()\n",
    "\n",
    "        print(f'''[状态：{crawler_status}，尝试次数：{b}] - {input_.loc[a, 'No']}.{input_.loc[a, 'ASIN']}\\n[剩余数量：{work.qsize()}] - [当前时间：{datetime.now().strftime('%H:%M:%S')}]\\n''')\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "list_task = []\n",
    "for _ in range(12):\n",
    "    task = gevent.spawn(crawler)\n",
    "    list_task.append(task)\n",
    "gevent.joinall(list_task)\n",
    "\n",
    "print('输出ing...')\n",
    "print()\n",
    "if not output_correct.empty:\n",
    "    output_correct = output_correct.sort_values(by=['No'],\n",
    "                                                ascending=[True],\n",
    "                                                ignore_index=True)\n",
    "    output_correct.to_excel(f'''./3.part-{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx''', index=False)\n",
    "if not output_error.empty:\n",
    "    output_error = output_error.sort_values(by=['No'],\n",
    "                                            ascending=[True],\n",
    "                                            ignore_index=True)\n",
    "    output_error.to_excel(f'./part_error.xlsx', index=False)\n",
    "    print('爬虫存在error')\n",
    "    print()\n",
    "print('Done ~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46198608-f2b0-4fba-88a2-897ca28130a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
