{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a02e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a866349",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dir = 'data_20240206'\n",
    "\n",
    "list_dir = list(os.walk('./' + param_dir))[0][1]\n",
    "if '.DS_Store' in list_dir:\n",
    "    list_dir.remove('.DS_Store')\n",
    "if '0.null.txt' in list_dir:\n",
    "    list_dir.remove('0.null.txt')\n",
    "    \n",
    "list_dir.sort()\n",
    "    \n",
    "print(len(list_dir))\n",
    "list_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c398ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for param_make in ['DU PONT',\n",
    " 'VOLKSWAGEN',\n",
    " 'PORSCHE',\n",
    " 'HINO',\n",
    " 'STUDEBAKER',\n",
    " 'TURNER',\n",
    " 'DUAL-GHIA',\n",
    " 'STEARNS KNIGHT',\n",
    " 'BUGATTI',\n",
    " 'DIANA',\n",
    " 'BERKELEY',\n",
    " 'AMPHICAR',\n",
    " 'MITSUBISHI',\n",
    " 'ALPINE',\n",
    " 'HUMMER',\n",
    " 'RENAULT',\n",
    " 'AUSTIN',\n",
    " 'WHIPPET',\n",
    " 'HRG',\n",
    " 'MERCURY',\n",
    " 'GMC',\n",
    " 'CUPRA',\n",
    " 'HUDSON',\n",
    " 'STERLING',\n",
    " 'FERRARI',\n",
    " 'SWALLOW',\n",
    " 'REO',\n",
    " 'FORD',\n",
    " 'AMERICAN AUSTIN',\n",
    " 'THINK',\n",
    " 'LINCOLN',\n",
    " 'PONTIAC',\n",
    " 'CHANGAN',\n",
    " 'ARNOLT-MG',\n",
    " 'DEUTSCH-BONNET',\n",
    " 'RILEY',\n",
    " 'PIERCE-ARROW',\n",
    " 'CUNNINGHAM',\n",
    " 'AC',\n",
    " 'CROSLEY',\n",
    " 'TRIUMPH',\n",
    " 'ISUZU',\n",
    " 'SCION',\n",
    " 'HISPANO-SUIZA',\n",
    " 'MCLAREN',\n",
    " 'DAEWOO',\n",
    " 'MERKUR',\n",
    " 'UAZ',\n",
    " 'DETOMASO',\n",
    " 'BAIC',\n",
    " 'ISO',\n",
    " 'ZUNDAPP',\n",
    " 'ELCAR',\n",
    " 'MINI',\n",
    " 'JEEP',\n",
    " 'VESPA',\n",
    " 'MAZDA',\n",
    " 'GRAHAM-PAIGE',\n",
    " 'WILLYS',\n",
    " 'MG',\n",
    " 'MASTRETTA',\n",
    " 'GLAS',\n",
    " 'OSCA',\n",
    " 'CORD',\n",
    " 'BENTLEY',\n",
    " 'DAIHATSU',\n",
    " 'DESOTO',\n",
    " 'MARMON',\n",
    " 'LASALLE',\n",
    " 'MESSERSCHMITT',\n",
    " 'CHECKER',\n",
    " 'MARCOS',\n",
    " 'AVANTI',\n",
    " 'WESTCOTT',\n",
    " 'FOTON',\n",
    " 'JMC',\n",
    " 'AUBURN',\n",
    " 'MACK',\n",
    " 'LAMBORGHINI',\n",
    " 'STUTZ',\n",
    " 'BOND',\n",
    " 'SUNBEAM',\n",
    " 'VPG',\n",
    " 'KAISER-FRAZER',\n",
    " 'JOWETT',\n",
    " 'ASUNA',\n",
    " 'DELLOW',\n",
    " 'FAW',\n",
    " 'AUSTIN-HEALEY',\n",
    " 'CHIREY',\n",
    " 'BUICK',\n",
    " 'MORETTI',\n",
    " 'MOON',\n",
    " 'FISKER',\n",
    " 'LEXINGTON',\n",
    " 'TVR',\n",
    " 'DAF',\n",
    " 'SMART',\n",
    " 'DATSUN',\n",
    " 'PEERLESS',\n",
    " 'AMERICAN MOTORS',\n",
    " 'ELVA',\n",
    " 'MORRIS',\n",
    " 'WARTBURG',\n",
    " 'DELAHAYE',\n",
    " 'LOTUS',\n",
    " 'LAND ROVER',\n",
    " 'SAAB',\n",
    " 'GEO',\n",
    " 'MARATHON',\n",
    " 'KURTIS',\n",
    " 'TESLA',\n",
    " 'EAGLE',\n",
    " 'HYUNDAI',\n",
    " 'UTILIMASTER',\n",
    " 'KIA',\n",
    " 'FACEL VEGA',\n",
    " 'ARNOLT-BRISTOL',\n",
    " 'WORKHORSE',\n",
    " 'OPEL',\n",
    " 'MATRA',\n",
    " 'SRT',\n",
    " 'GRAHAM',\n",
    " 'CHEVROLET',\n",
    " 'SATURN',\n",
    " 'BRISTOL',\n",
    " 'LAFORZA',\n",
    " 'DAIMLER',\n",
    " 'HOTCHKISS',\n",
    " 'MAYBACH',\n",
    " 'ABARTH',\n",
    " 'IVECO',\n",
    " 'SALEEN',\n",
    " 'GORDON-KEEBLE',\n",
    " 'INFINITI',\n",
    " 'FARGO',\n",
    " 'PASSPORT',\n",
    " 'TALBOT-LAGO',\n",
    " 'ALLSTATE',\n",
    " 'TOYOTA',\n",
    " 'NSU',\n",
    " 'STANDARD',\n",
    " 'JEWETT',\n",
    " 'TATRA',\n",
    " 'PEGASO',\n",
    " 'YUGO',\n",
    " 'POLESTAR',\n",
    " 'UD',\n",
    " 'CADILLAC',\n",
    " 'NISSAN',\n",
    " 'COLE',\n",
    " 'MOBILITY VENTURES',\n",
    " 'ARMSTRONG-SIDDELEY',\n",
    " 'DODGE',\n",
    " 'ACURA',\n",
    " 'MONTEVERDI',\n",
    " 'DUESENBERG',\n",
    " 'LEXUS',\n",
    " 'MITSUBISHI FUSO',\n",
    " 'ALVIS',\n",
    " 'QVALE',\n",
    " 'STAR',\n",
    " 'FRAZER NASH',\n",
    " 'KARMA',\n",
    " 'AUDI',\n",
    " 'LADA',\n",
    " 'STEVENS-DURYEA',\n",
    " 'PLYMOUTH',\n",
    " 'VOLVO',\n",
    " 'EXCALIBUR',\n",
    " 'ESSEX',\n",
    " 'PEUGEOT',\n",
    " 'SUBARU',\n",
    " 'LEA-FRANCIS',\n",
    " 'DURANT',\n",
    " 'SABRA',\n",
    " 'BORGWARD',\n",
    " 'PAIGE',\n",
    " 'APOLLO',\n",
    " 'VAUXHALL',\n",
    " 'ROAMER',\n",
    " 'GOLIATH',\n",
    " 'PANHARD',\n",
    " 'LUCID',\n",
    " 'SHELBY',\n",
    " 'PANOZ',\n",
    " 'GIANT MOTORS',\n",
    " 'BIZZARRINI',\n",
    " 'FRANKLIN',\n",
    " 'PETERBILT',\n",
    " 'FALCON KNIGHT',\n",
    " 'DENZEL',\n",
    " 'NARDI',\n",
    " 'ROCKNE',\n",
    " 'ALFA ROMEO',\n",
    " 'RIVIAN',\n",
    " 'AMERICAN BANTAM',\n",
    " 'ASTON MARTIN',\n",
    " 'BRICKLIN',\n",
    " 'MAICO',\n",
    " 'HENRY J',\n",
    " 'HILLMAN',\n",
    " 'GRIFFITH',\n",
    " 'FLINT',\n",
    " 'PACKARD',\n",
    " 'JENSEN',\n",
    " 'CASE',\n",
    " 'MARAUDER',\n",
    " 'SKODA',\n",
    " 'SSANGYONG',\n",
    " 'KISSEL',\n",
    " 'SALMSON',\n",
    " 'OLDSMOBILE',\n",
    " 'WINDSOR',\n",
    " 'DORETTI',\n",
    " 'ALLARD',\n",
    " 'CHRYSLER',\n",
    " 'MASERATI',\n",
    " 'JAGUAR',\n",
    " 'PANTHER',\n",
    " 'OMEGA',\n",
    " 'KENWORTH',\n",
    " 'WOLSELEY',\n",
    " 'SIATA',\n",
    " 'RICKENBACKER',\n",
    " 'MORGAN',\n",
    " 'DKW',\n",
    " 'FAIRTHORPE',\n",
    " 'BMW',\n",
    " 'HUMBER',\n",
    " 'FIAT',\n",
    " 'CONTINENTAL',\n",
    " 'MAXWELL',\n",
    " 'LANCHESTER',\n",
    " 'AM GENERAL',\n",
    " 'RELIANT',\n",
    " 'MERCEDES-BENZ',\n",
    " 'DELOREAN',\n",
    " 'JAC',\n",
    " 'ROVER',\n",
    " 'GARDNER',\n",
    " 'DELAGE',\n",
    " 'CITROEN',\n",
    " 'SINGER',\n",
    " 'HONDA',\n",
    " 'EDSEL',\n",
    " 'FREIGHTLINER',\n",
    " 'ROLLS-ROYCE',\n",
    " 'GENESIS',\n",
    " 'YELLOW CAB',\n",
    " 'VAM',\n",
    " 'SUZUKI',\n",
    " 'LAGONDA',\n",
    " 'HUPMOBILE',\n",
    " 'CISITALIA',\n",
    " 'SPYKER',\n",
    " 'ERSKINE',\n",
    " 'MOSKVICH',\n",
    " 'JORDAN',\n",
    " 'INTERNATIONAL',\n",
    " 'RAM',\n",
    " 'NASH',\n",
    " 'SEAT',\n",
    " 'ZACUA',\n",
    " 'OAKLAND',\n",
    " 'SIMCA',\n",
    " 'LANCIA',\n",
    " 'HEALEY']:\n",
    "\n",
    "    from gevent import monkey\n",
    "    monkey.patch_all(thread=False)\n",
    "    from gevent.queue import Queue\n",
    "    import gevent\n",
    "\n",
    "    import sys\n",
    "    sys.path.append('../../../00.module')\n",
    "    import UA\n",
    "    import Proxy\n",
    "\n",
    "    # = = = = = = = = = = = = = = =\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    output_correct = pd.DataFrame()\n",
    "\n",
    "    output_error = pd.DataFrame()\n",
    "\n",
    "    input_ = pd.read_excel('./' + param_dir + '/' + param_make + '/4.engine.xlsx',\n",
    "                           header=0,\n",
    "                           dtype=str).fillna('')\n",
    "\n",
    "    length = len(input_)\n",
    "\n",
    "    print('总数量：' + str(length))\n",
    "    print()\n",
    "\n",
    "    # = = = = = = = = = = = = = = =\n",
    "\n",
    "    import requests\n",
    "    import json\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    from lxml import etree\n",
    "\n",
    "    # = = = = = = = = = = = = = = =\n",
    "\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "\n",
    "    # = = = = = = = = = = = = = = =\n",
    "\n",
    "    work = Queue()\n",
    "    for i in range(length):\n",
    "        work.put_nowait(i)\n",
    "\n",
    "    def crawler():\n",
    "        global output_correct, output_error\n",
    "\n",
    "        while not work.empty():\n",
    "            i = work.get_nowait()\n",
    "\n",
    "            # = = = = = = = = = = = = = = =\n",
    "\n",
    "            crawler_status = 'error'\n",
    "            for _ in range(101):\n",
    "                try:\n",
    "                    jsn = '{\"jsn\":{\"tab\":\"catalog\",\"make\":\"' + input_.loc[i, 'Make'] + '\",\"year\":\"' + input_.loc[i, 'Year'] + '\",\"model\":\"' + input_.loc[i, 'Model'] + '\",\"engine\":\"' + input_.loc[i, 'Engine'] + '\",\"carcode\":\"' + input_.loc[i, 'Car_Code'] + '\",\"nodetype\":\"carcode\"}}'\n",
    "\n",
    "                    payload = {'func': 'navnode_fetch',\n",
    "                               'payload': jsn,\n",
    "                               'api_json_request': '1'}\n",
    "\n",
    "                    resp = requests.post('https://www.rockauto.com/catalog/catalogapi.php',\n",
    "                                         data=payload,\n",
    "                                         headers=UA.get_User_Agent_Requests(),\n",
    "                                         proxies=Proxy.get_Proxy_Requests()).text\n",
    "\n",
    "                    resp = json.loads(resp)\n",
    "\n",
    "                    # = = = = = = = = = = = = = = =\n",
    "\n",
    "                    html = resp['html_fill_sections']['navchildren[]']\n",
    "\n",
    "                    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "                    # = = = = = = = = = = = = = = =\n",
    "\n",
    "                    html = etree.HTML(str(soup))\n",
    "\n",
    "                    list_category = [category.strip() for category in html.xpath('//td[@class=\"nlabel\"]/a/text()')]\n",
    "\n",
    "                    # = = = = = = = = = = = = = = =\n",
    "\n",
    "                    list_url = ['https://www.rockauto.com'+url.strip() for url in html.xpath('//td[@class=\"niconspace ncollapsedicon\"]/a/@href')]\n",
    "\n",
    "                    # = = = = = = = = = = = = = = =\n",
    "\n",
    "                    crawler_status = 'ok'\n",
    "\n",
    "                    break\n",
    "\n",
    "                except:\n",
    "                    time.sleep(0.3)\n",
    "                    continue\n",
    "\n",
    "            if crawler_status == 'ok':\n",
    "                df_temp = pd.DataFrame({'Tab_Label': input_.loc[i, 'Tab_Label'],\n",
    "                                        'No.': int(input_.loc[i, 'No.']),\n",
    "                                        'Make': input_.loc[i, 'Make'],\n",
    "                                        'Year': input_.loc[i, 'Year'],\n",
    "                                        'Model': input_.loc[i, 'Model'],\n",
    "                                        'Engine': input_.loc[i, 'Engine'],\n",
    "                                        'Car_Code': input_.loc[i, 'Car_Code'],\n",
    "                                        'Category': list_category,\n",
    "                                        'Url': list_url})\n",
    "\n",
    "                output_correct = pd.concat([output_correct, df_temp], ignore_index=True).fillna('')\n",
    "            else:\n",
    "                df_temp = pd.DataFrame([{'Tab_Label': input_.loc[i, 'Tab_Label'],\n",
    "                                         'No.': int(input_.loc[i, 'No.']),\n",
    "                                         'Make': input_.loc[i, 'Make'],\n",
    "                                         'Year': input_.loc[i, 'Year'],\n",
    "                                         'Model': input_.loc[i, 'Model'],\n",
    "                                         'Engine': input_.loc[i, 'Engine'],\n",
    "                                         'Car_Code': input_.loc[i, 'Car_Code'],\n",
    "                                         'Url': input_.loc[i, 'Url']}])\n",
    "\n",
    "                output_error = pd.concat([output_error, df_temp], ignore_index=True).fillna('')\n",
    "\n",
    "            print(input_.loc[i, 'Make'] + ' - ' + input_.loc[i, 'Year'] + ' - ' + input_.loc[i, 'Model'] + ' - ' +  input_.loc[i, 'Engine'] + '  <->  [' + crawler_status + '] - 剩余数量：' + str(work.qsize()) + ' - 当前时间：' + datetime.now().strftime('%H:%M:%S'))\n",
    "\n",
    "    # = = = = = = = = = = = = = = =\n",
    "\n",
    "    list_task = []\n",
    "    for _ in range(20):\n",
    "        task = gevent.spawn(crawler)\n",
    "        list_task.append(task)\n",
    "    gevent.joinall(list_task)\n",
    "\n",
    "    print()\n",
    "    print('爬虫输出中')\n",
    "    output_correct = output_correct.drop_duplicates(ignore_index=True)\n",
    "    output_correct = output_correct.sort_values(by=['No.', 'Year', 'Model', 'Engine', 'Category'], ascending=[True, False, True, True, True])\n",
    "    output_correct.to_excel('./' + param_dir + '/' + param_make + '/5.category-' + datetime.now().strftime('%Y%m%d_%H%M%S') + '.xlsx', index=False)\n",
    "    if len(output_error) != 0:\n",
    "        output_error = output_error.drop_duplicates(ignore_index=True)\n",
    "        output_error = output_error.sort_values(by=['No.', 'Year', 'Model', 'Engine'], ascending=[True, False, True, True])\n",
    "        output_error.to_excel('./' + param_dir + '/' + param_make + '/category_error.xlsx', index=False)\n",
    "        print()\n",
    "        print('爬虫存在error')\n",
    "    print()\n",
    "    print('爬虫结束')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd365165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总数量：13\n",
      "\n",
      "FORD - 1975 - F-100 - 5.0L 302cid V8  <->  [ok] - 剩余数量：0 - 当前时间：19:25:44\n",
      "FORD - 1980 - F-800 - 8.2L 501cid V8 DIESEL Turbocharged  <->  [ok] - 剩余数量：0 - 当前时间：19:25:44\n",
      "FORD - 1976 - COUNTRY SQUIRE - 5.8L 351cid V8  <->  [ok] - 剩余数量：0 - 当前时间：19:25:44\n",
      "FORD - 1976 - E-350 - 5.8L 351cid V8  <->  [ok] - 剩余数量：0 - 当前时间：19:25:44\n",
      "FORD - 1990 - L SERIES TRUCK - 14.0L 855cid L6 DIESEL Turbocharged  <->  [ok] - 剩余数量：0 - 当前时间：19:25:44\n",
      "FORD - 1981 - BRONCO - 5.0L 302cid V8  <->  [ok] - 剩余数量：0 - 当前时间：19:25:44\n",
      "FORD - 1987 - C SERIES TRUCK - 10.4L 636cid V8 DIESEL  <->  [ok] - 剩余数量：0 - 当前时间：19:25:44\n",
      "FORD - 1979 - F-150 - 6.6L 400cid V8  <->  [ok] - 剩余数量：0 - 当前时间：19:25:44\n",
      "FORD - 1976 - F-150 - 7.5L 460cid V8  <->  [ok] - 剩余数量：0 - 当前时间：19:25:44\n",
      "FORD - 1988 - C SERIES TRUCK - 8.2L 501cid V8 DIESEL Turbocharged  <->  [ok] - 剩余数量：0 - 当前时间：19:25:44\n",
      "FORD - 1979 - F-250 - 7.5L 460cid V8  <->  [ok] - 剩余数量：0 - 当前时间：19:25:45\n",
      "FORD - 1983 - B SERIES BUS - 8.2L 501cid V8 DIESEL  <->  [ok] - 剩余数量：0 - 当前时间：19:25:45\n",
      "FORD - 1973 - LTD - 7.5L 460cid V8  <->  [ok] - 剩余数量：0 - 当前时间：19:25:47\n",
      "\n",
      "爬虫输出中\n",
      "\n",
      "爬虫结束\n"
     ]
    }
   ],
   "source": [
    "from gevent import monkey\n",
    "monkey.patch_all(thread=False)\n",
    "from gevent.queue import Queue\n",
    "import gevent\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../00.module')\n",
    "import UA\n",
    "import Proxy\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "output_correct = pd.DataFrame()\n",
    "\n",
    "output_error = pd.DataFrame()\n",
    "\n",
    "input_ = pd.read_excel('./category_error.xlsx',\n",
    "                       header=0,\n",
    "                       dtype=str).fillna('')\n",
    "\n",
    "length = len(input_)\n",
    "\n",
    "print('总数量：' + str(length))\n",
    "print()\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "from urllib.parse import quote\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from lxml import etree\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "work = Queue()\n",
    "for i in range(length):\n",
    "    work.put_nowait(i)\n",
    "\n",
    "def crawler():\n",
    "    global output_correct, output_error\n",
    "\n",
    "    while not work.empty():\n",
    "        i = work.get_nowait()\n",
    "\n",
    "        # = = = = = = = = = = = = = = =\n",
    "\n",
    "        crawler_status = 'error'\n",
    "        for _ in range(101):\n",
    "            try:\n",
    "                jsn = '{\"jsn\":{\"tab\":\"catalog\",\"make\":\"' + input_.loc[i, 'Make'] + '\",\"year\":\"' + input_.loc[i, 'Year'] + '\",\"model\":\"' + input_.loc[i, 'Model'] + '\",\"engine\":\"' + input_.loc[i, 'Engine'] + '\",\"carcode\":\"' + input_.loc[i, 'Car_Code'] + '\",\"nodetype\":\"carcode\"}}'\n",
    "\n",
    "                payload = {'func': 'navnode_fetch',\n",
    "                           'payload': jsn,\n",
    "                           'api_json_request': '1'}\n",
    "\n",
    "                resp = requests.post('https://www.rockauto.com/catalog/catalogapi.php',\n",
    "                                     data=payload,\n",
    "                                     headers=UA.get_User_Agent_Requests(),\n",
    "                                     proxies=Proxy.get_Proxy_Requests()).text\n",
    "\n",
    "                resp = json.loads(resp)\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                html = resp['html_fill_sections']['navchildren[]']\n",
    "\n",
    "                soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                html = etree.HTML(str(soup))\n",
    "\n",
    "                list_category = [category.strip() for category in html.xpath('//td[@class=\"nlabel\"]/a/text()')]\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                list_url = ['https://www.rockauto.com'+url.strip() for url in html.xpath('//td[@class=\"niconspace ncollapsedicon\"]/a/@href')]\n",
    "\n",
    "                # = = = = = = = = = = = = = = =\n",
    "\n",
    "                crawler_status = 'ok'\n",
    "\n",
    "                break\n",
    "\n",
    "            except:\n",
    "                time.sleep(0.3)\n",
    "                continue\n",
    "\n",
    "        if crawler_status == 'ok':\n",
    "            df_temp = pd.DataFrame({'Tab_Label': input_.loc[i, 'Tab_Label'],\n",
    "                                    'No.': int(input_.loc[i, 'No.']),\n",
    "                                    'Make': input_.loc[i, 'Make'],\n",
    "                                    'Year': input_.loc[i, 'Year'],\n",
    "                                    'Model': input_.loc[i, 'Model'],\n",
    "                                    'Engine': input_.loc[i, 'Engine'],\n",
    "                                    'Car_Code': input_.loc[i, 'Car_Code'],\n",
    "                                    'Category': list_category,\n",
    "                                    'Url': list_url})\n",
    "\n",
    "            output_correct = pd.concat([output_correct, df_temp], ignore_index=True).fillna('')\n",
    "        else:\n",
    "            df_temp = pd.DataFrame([{'Tab_Label': input_.loc[i, 'Tab_Label'],\n",
    "                                     'No.': int(input_.loc[i, 'No.']),\n",
    "                                     'Make': input_.loc[i, 'Make'],\n",
    "                                     'Year': input_.loc[i, 'Year'],\n",
    "                                     'Model': input_.loc[i, 'Model'],\n",
    "                                     'Engine': input_.loc[i, 'Engine'],\n",
    "                                     'Car_Code': input_.loc[i, 'Car_Code'],\n",
    "                                     'Url': input_.loc[i, 'Url']}])\n",
    "\n",
    "            output_error = pd.concat([output_error, df_temp], ignore_index=True).fillna('')\n",
    "\n",
    "        print(input_.loc[i, 'Make'] + ' - ' + input_.loc[i, 'Year'] + ' - ' + input_.loc[i, 'Model'] + ' - ' +  input_.loc[i, 'Engine'] + '  <->  [' + crawler_status + '] - 剩余数量：' + str(work.qsize()) + ' - 当前时间：' + datetime.now().strftime('%H:%M:%S'))\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "list_task = []\n",
    "for _ in range(20):\n",
    "    task = gevent.spawn(crawler)\n",
    "    list_task.append(task)\n",
    "gevent.joinall(list_task)\n",
    "\n",
    "print()\n",
    "print('爬虫输出中')\n",
    "output_correct = output_correct.drop_duplicates(ignore_index=True)\n",
    "output_correct = output_correct.sort_values(by=['No.', 'Year', 'Model', 'Engine', 'Category'], ascending=[True, False, True, True, True])\n",
    "output_correct.to_excel('./5.category-' + datetime.now().strftime('%Y%m%d_%H%M%S') + '.xlsx', index=False)\n",
    "if len(output_error) != 0:\n",
    "    output_error = output_error.drop_duplicates(ignore_index=True)\n",
    "    output_error = output_error.sort_values(by=['No.', 'Year', 'Model', 'Engine'], ascending=[True, False, True, True])\n",
    "    output_error.to_excel('./category_error_error.xlsx', index=False)\n",
    "    print()\n",
    "    print('爬虫存在error')\n",
    "print()\n",
    "print('爬虫结束')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519d9b90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
